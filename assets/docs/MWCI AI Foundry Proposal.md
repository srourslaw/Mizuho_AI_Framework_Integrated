1
Manila Water
Consortium
AI Foundry
Proposal

2
Contents
2
Confidential | www.tha kra lone.co m
Part 1: Our Approach
▪ Executive Summary
▪ Key Challenges
▪ 8 Steps to Address Key Challenges
▪ Success Factors
▪ Understanding of the Requirements
▪ Alignment to Project Outcomes
▪ Typical Engagement Timeline
Part 2: Our 8 Step Framework
Part 3: Architecture and Technical Delivery Approach
▪ Roadmap & Methodology
▪ Delivery Phases
▪ Solution Blueprints
▪ Platform & Core Components
▪ Key Architectural Patterns
▪ End to End CI/CD Pipeline Overview
▪ User Interface & Experience
▪ Core UI Modules
▪ Accessibility & Internationalization
▪ End to End UI Testing & Quality
▪ Data and Model Operations (MLOps)
▪ Sample Automated Retaining Workflow
▪ Security & Compliance
▪ Testing & Quality Assurance
Part 5: Timelines and Commercials
Part 6: About Thakral One
Appendix 1: 8 Step Framework Detailed Approach and Methodology
Appendix 2: Case Studies

Our Approach

Confidential | www.tha kra lone.co m
MWC AI Foundry: Executive Summary
4
Project Objective
Manila Water Company, Inc. (“MWCI”) seeks a partner to design, implement,
and operationalize an Artificial Intelligence Foundry (AIF) that accelerates
intelligent automation, enhances decision-making, and democratizes AI-
driven insights across the enterprise.
1. AI Foundry Framework A scalable architecture to support diverse AI
capabilities, including HR operations, IT support, and knowledge
management.
2. Governance and Security A robust focus on data governance, security,
ethical AI use, and compliance with privacy regulations such as GDPR and
PDPA.
3. Generative AI Use Cases AI-driven content generation, knowledge
synthesis, and AI integration into service desk operations to enhance
decision-making and employee productivity.
4. Integrations Seamless integration with existing systems (e.g., SAP
SuccessFactors, ManageEngine, and the enterprise data lake).
5. Model Development Development of NLP-based models for HR, ticketing
automation, and real-time data interaction through AI-driven querying.
6. Testing and Deployment Ensuring comprehensive testing (functional,
performance, security), user training, and a controlled phased deployment
for smooth project execution.
7. Continuous Improvement Implementing feedback loops for ongoing
optimization and scaling of AI capabilities based on real-world feedback.
Thakral One proposes a dual-track approach combining
1. Our proven, 8-step AI Adoption Framework to ensure strategic alignment,
governance, and sustained change management.
2. A robust technical architecture leveraging Microsoft Fabric Data Science
Workspaces, Azure AI services, and integrated enterprise systems (SAP
SuccessFactors, ManageEngine, SCADA, LIMS).
3. Our solution delivers four pillars of capability:
▪ Conversational AI for HR and IT self-service
▪ Generative AI for document intelligence and knowledge synthesis
▪ Semantic search & “Chat with Your Data” via RAG and vector search
▪ Predictive analytics & automation for proactive operations
Focus Areas
▪ HR, ITSM, and Knowledge Management as initial targets
▪ GenAI-powered Agents and "Chat with your Data" prototypes
▪ Cloud/on-premise Lakehouse architecture and MLOps
▪ Responsible AI governance and user-centric adoption
Key Benefits
▪ 60% reduction in HR inquiry resolution time
▪ 70% first-contact resolution in ITSM
▪ 85% accuracy in natural language queries over Lakehouse data
▪ 150–200% cumulative ROI over three years
Enable enterprise-wide AI capability through a structured AI Foundry initiative.

Together we will drive a full-spectrum, structured path to scalable, sustainable AI transformation.
Confidential | www.tha kra lone.co m
Enterprise AI Adoption: Executive Summary
5
Across industries, AI is no longer a futuristic concept, it’s a
competitive necessity.
▪ Organizations are adopting AI to unlock new revenue streams,
enhance customer experience, automate processes,
and drive data-driven decision-making with the clear intention
to improve efficiency & accuracy by reducing human costs and
errors.
▪ Organizations in the top maturity tier report 15–30%
improvements in productivity, retention, and customer
experience—and companies that truly scale AI see ~3× higher
revenue impact and ~30% EBIT uplift compared to pilot-only firms.
▪ Despite growing urgency, less than 15% of enterprise AI
initiatives reach sustainable production, and most firms cite a
mix of technical, organizational, and cultural barriers.
Many companies pilot AI, few achieve scale.
AI at scale isn’t just about technology, it’s about aligning
strategy, data, people, and change.
We help you bridge the gap between innovation & enterprise scalability
using our 8-step AI adoption framework, which directly targets the most
common and costly blockers.
1. Enterprise AI Adoption Strategy
Aligns AI initiatives with business strategy to drive measurable outcomes across the
enterprise.
2. AI Maturity Assessment & Ladder
Evaluates current capabilities and sets a clear roadmap to scale AI effectively.
3. AI Technology & Data Foundations
Builds scalable, secure data and infrastructure foundations to support enterprise-
grade AI.
4. AI Governance Framework
Establishes ethical, risk-aware, and compliant guardrails for responsible AI adoption.
5. AI Prototype Driven Use Cases
Delivers quick, value-focused AI pilots using real data to prove business impact.
6. Embedding Meaningful AI
Integrates successful AI solutions into core operations and enterprise systems.
7. AI – Human Factors
Equips teams with skills, tools, and change management to ensure AI adoption sticks.
8. AI Continuous Innovation
Creates a sustainable AI engine through MLOps, CoE, and a pipeline of new use
cases.

Confidential | www.tha kra lone.co m 
6
1. Lack of strategic alignment & weak executive sponsorship
Many organizations struggle to align AI initiatives with business goals due to
poor executive sponsorship or unclear vision. A McKinsey survey found only
21% of companies have an enterprise-wide AI strategy, and just 30%
report strong executive backing for AI programs.
2. Enterprises overestimate their AI maturity
Without a clear view of their current capabilities, organizations make
misguided investments. Gartner reports that 60% of AI projects stall at the
proof-of-concept stage due to unrealistic expectations and unaddressed
maturity gaps.
3. Data fragmentation & poor quality limit AI performance
Siloed, inconsistent, and incomplete data remain a core blocker for enterprise
AI. According to MIT Sloan, 42% of firms cite lack of high-quality
proprietary data, and 48% are concerned about accuracy or bias in AI
outputs due to poor data foundations.
4. Ethical, compliance & governance concerns slow adoption
Fear of regulatory breaches, reputational risk, or algorithmic bias causes
hesitation. IBM research shows over 40% of firms are concerned about
explainability and regulatory compliance, especially in industries like
finance and healthcare.
5. Pilots often fail to scale beyond experimentation
Promising AI proofs of concept frequently get stuck in “pilot purgatory.” BCG
notes that only 15% of AI pilots reach production, largely due to a lack of
scalable frameworks and operational readiness.
6. Embedding AI into operations is technically challenging
Legacy systems, disjointed workflows, and lack of DevOps/MLOps capabilities
hamper deployment. According to Deloitte, 47% of organizations face
integration barriers when scaling AI into real-world processes.
7. Skills shortages and resistance to change inhibit adoption
AI talent remains scarce, and employee fear of automation creates internal
friction. World Economic Forum data indicates 42% of firms struggle to find
skilled AI professionals, while up to 70% of AI transformation success
depends on effective change management.
8. Momentum fades post-launch without a clear innovation
engine
After initial pilots, many firms lack structured processes to sustain innovation.
Accenture finds that just 29% of organizations continuously scale AI use
cases, and most don’t invest in long-term talent or pipeline-building
mechanisms.
Enterprise AI Adoption: Key Challenges
Despite the growing urgency to adopt AI, many enterprises encounter recurring roadblocks that stall or derail
transformation efforts. These challenges can be grouped into 8 problem areas:

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: 8 Steps to Address Key Challenges
7
1. Lack of Clear AI Vision & Executive Alignment
Only 21% of firms report having a unified AI strategy.
Addressed in Step 1: Enterprise AI Adoption Strategy establishing a CEO-
sponsored AI roadmap tied to business goals, prioritized use cases, and cross-
department alignment.
2. Unclear AI Maturity and Capability Gaps
60% of AI projects fail due to unassessed gaps in readiness, skills, and process maturity.
Addressed in Step 2: AI Maturity Assessment & Ladder delivering a maturity
assessment with actionable roadmap to close capability and infrastructure gaps.
3. Fragmented, Low-Quality Data & Infrastructure
42% of companies lack sufficient proprietary data; poor data quality blocks AI training
and trust.
Addressed in Step 3: AI Technology & Data Foundations delivering a
compliant, scalable data platform with quality pipelines, integrated across silos.
4. Risk, Bias & Ethical Concerns in AI Use
Over 40% cite regulatory, privacy, and bias issues as major deployment blockers.
Addressed in Step 4: AI Governance Framework developing a tailored AI
governance framework ensuring ethical, explainable, and auditable AI.
5. Pilots Fail to Reach Scale ("Pilot Purgatory")
Only 15% of pilots move to production; lack of validation, sponsorship, or change
enablement.
Addressed in Step 5: AI Prototype Driven Use Cases delivering business-
validated, real-data prototypes that drive confidence and investment for scale.
6. Barriers to Deployment & Operational Integration
47% of enterprises struggle to integrate AI into core systems or workflows.
Addressed in Step 6: Embedding Meaningful AI enabling operational AI
embedded into business processes, with IT support and scalability.
7. Skills Gaps, Cultural Resistance, Change Fatigue
42% cite AI talent gaps; success depends 70% on managing people-related factors
Addressed in Step 7: AI – Human Factors effectively training teams,
redefining roles, CoE support, and an adoption-ready workforce.
8. Loss of Momentum After Initial Success
Only 29% maintain innovation after early AI wins.
Addressed in Step 8: AI Continuous Innovation by standing up a sustainable
CoE, innovation pipeline, and continuous model monitoring to scale impact.
We drive a structured, 8-stage consulting-led approach to help enterprises transition from AI experimentation to
enterprise-wide impact, with governance, change management, and technical depth built-in.

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: Our Success Factor
8
Our success factor is our 8 step prototype-led, co-creation
model that blends strategic consulting with real-data
experimentation, accelerating trust, adoption, and value delivery.
At our core we are technologists, builders, data and AI experts.
Co-Creation With Clients
We design AI systems with users, not for
them—through joint discovery, iterative
prototyping, and embedded pilots.
Our prototypes use your live datasets to
validate impact early—avoiding theoretical
“PowerPoint AI.”
▪ Technology Consulting: Systems
integration, cloud, MLOps
▪ Data & Analytics: Pipelines, quality,
governance
▪ AI Labs: Fast-turnaround proof-of-
concepts and experimentation
Cross-Disciplinary Strength
1. Strategic Alignment from Day One
Ensure AI initiatives are linked to business goals, with executive sponsorship
driving enterprise-wide traction.
2. Maturity-Based Planning
Diagnose current readiness and gaps across people, data, and tech, enabling a
tailored roadmap to success.
3. Robust Data & Tech Foundations
Build scalable, AI-ready infrastructure and data pipelines, eliminating
fragmentation and accelerating model development.
4. Governance-First Approach
Embed responsible AI principles early—covering ethics, privacy, risk, and
compliance—boosting trust and auditability.
5. Prototype-Led Validation
Focuss on real-data use cases to prove value fast, reduce risk, and gain early
stakeholder buy-in.
6. Operational Integration by Design
Move beyond pilots by embedding AI into existing workflows, systems, and
decision processes for real business impact.
7. Human-Centric Change Management
Address adoption risks by upskilling teams, managing resistance, and nurturing a
collaborative AI culture.
8. Built-In Innovation Engine
Establish ongoing MLOps, use case pipelines, and a CoE to scale, sustain, and
evolve AI capabilities over time.
Real Data, Real Value

Confidential | www.tha kra lone.co m
Understanding of the Requirements
9
▪ Ethical AI Framework: Bias
detection, human-in-the-loop
approval for high-risk decisions,
model explainability (SHAP/LIME).
▪ Privacy & Data Protection:
PDPA/GDPR compliance, data
classification, masking, consent
management, privacy-by-design.
▪ Audit & Monitoring: End-to-end
audit trails (Purview lineage,
MLflow logs, SIEM), automated
compliance reporting, quarterly
reviews.
▪ Platform Compatibility: Leverage
existing Microsoft Fabric
Lakehouse and SAP
SuccessFactors investments
▪ Data Ingestion: Batch and real-
time pipelines from SAP,
ManageEngine, SCADA, LIMS,
and other MWCI systems.
▪ Generative AI & RAG: Integration
of Azure OpenAI LLMs, custom
vector search, and SQL translation
for Lakehouse data.
▪ Infrastructure & Ops: Private
VNets, multi-region DR, Fabric
workspaces, Azure ML for
production serving.
▪ UX/UI Standards: Accessible,
multilingual interfaces (English,
Tagalog), seamless integration with
MS Teams and Power BI.
▪ Adoption & Training: Role-based
training (executive, power user, IT
admin), AI Champions network,
pulse surveys, and feedback loops.
Business Objectives 
Governance, Security
& Compliance
Technical Integration 
User Experience & Change
Management
▪ Executive Alignment: CEO-
sponsored AI governance model,
steering committee, and funding
mechanism.
▪ Value Focus: Target efficiency gains
(HR, ITSM), cost avoidance (risk,
forecasting), and revenue
opportunities (data monetization).
▪ Scalability: From pilot to enterprise-
wide rollouts, with capability to
onboard 10+ use cases per year.
1 2 3 4
MWCI’s TOR outlines a strategic directive to institutionalize AI across business functions, embed responsible AI practices, and
deliver measurable operational outcomes. We distilled the requirements into four core pillars and detailed sub-requirements

Confidential | www.tha kra lone.co m
MWC AI Foundry: How We Will Work With You
10
Co-Creation with the Business
What we do: Work side-by-side with HR, ITSM, and Knowledge
teams to design AI solutions grounded in real processes and
needs.
Why it matters: This ensures faster buy-in, practical relevance,
and adoption. No “black-box” consulting—every step is done with
your teams.
Prototype-Driven Approach Using Real Data
What we do: Build working AI prototypes on Manila Water’s own
data within weeks—not just PowerPoint or slideware.
Why it matters: Drives faster feedback, tangible results, and
avoids “pilot purgatory.” We show impact before scaling.
Deep Tech + Advisory Integration
What we do: Combine technology consulting, data architecture,
and ML engineering under one team.
Why it matters: Many firms separate strategy from execution—
our unified team ensures strategic alignment and technical
delivery without handovers.
Our success factor is our prototype-led, co-creation model that blends strategic consulting with real-data
experimentation, accelerating trust, adoption, and value delivery.
Step AI Adoption Step Mapping Your Key Requirements to our 8 Step Framework
1 Enterprise AI Adoption Strategy
▪ Stakeholder workshops
▪ Use case rationalization (HR, ITSM, Lakehouse)
▪ Vision for Agent AI, “Chat with your Data”, LLMs
2 AI Maturity Assessment & Ladder 
▪ Assess current systems, platforms, and data readiness
▪ Identify integration points & platform capabilities
3 AI Technology & Data Foundation
▪ Design modular AI Foundry architecture
▪ Lakehouse integration
▪ Cloud/on-premise infra assessment
▪ Partner tech strategy
4 AI Governance Framework
▪ Ethical AI governance & policy definitions
▪ HITL model validation
▪ Access control, role policies, consent management, privacy, audit
5 AI Prototype Driven Use Cases 
▪ Build MVPs for HR AI agent, IT ticketing models, “Chat with Data”
▪ Prompt engineering, RAG pipelines, fine-tuning LLMs
6 Embed Meaningful AI
▪ Pilot rollout + production deployment
▪ Real-time semantic search and GenAI agent integration into Teams
▪ SLA impact on HR/IT
7 AI – Human Factors
▪ User training and change management
▪ Create adoption material, run workshops
▪ Multilingual UIs, accessibility features
8 AI Continuous Innovation
▪ Model performance monitoring
▪ Feedback loops & optimizations
▪ Create AI CoE model registry, continuous improvement plan

Confidential | www.tha kra lone.co m
Using our 8 Step Framework to Strengthen Project Outcomes
11
Very good alignment of Manila Water’s requirements to our 8 Step Framework indicating deep thought and
serious intention of scaling AI enterprise wide.
8 Step Framework Alignment Challenge Addressed Manila Water’s Strategic Intent How Our Framework Strengthens It
1. Enterprise AI Adoption Strategy 
Lack of clear AI vision &
executive alignment
Executive Commitment to AI Transformation – Top-level
ambition to establish an “AI Foundry” signals institutional buy-in
and strategic intent.
Translate vision into execution with a structured AI roadmap,
KPIs, and governance-backed investment plan.
2. AI Maturity Assessment & Ladder 
Unclear AI maturity &
readiness gaps
Recognition of Capability Gaps – Acknowledges the need for
external expertise and targeted systems integration to accelerate
AI readiness.
Assess maturity across people, process, and technology to pace
investments and scale responsibly.
3. AI Technology & Data Foundations 
Data fragmentation &
quality issues
Focus on Foundational Architecture – Calls for integration
across SAP, ManageEngine, and the Data Lake to enable a
scalable enterprise AI platform.
Build scalable data foundations via modular pipelines and cloud-
native AI-ready platforms, ensuring interoperability, security, and
quality.
4. AI Governance Framework 
Risk, bias, regulatory &
ethical concerns
Proactive Security & Compliance Posture – Incorporates PDPA,
data privacy, and system auditing into early planning to safeguard
responsible AI adoption.
Implement Responsible AI governance with risk assessment,
bias testing, explainability, and lifecycle monitoring.
5. AI Prototype Driven Use Cases Pilots failing to scale
Openness to Applied AI Use Cases – Seeks HR and ITSM
chatbots, knowledge retrieval, and generative AI pilots to test real-
world business value.
Develop high-impact prototypes on real Manila Water data with
measurable KPIs, iterative feedback, and clear scaling pathways.
6. Embedding Meaningful AI 
Integration & deployment
barriers
Commitment to End-User Integration – Specifies links to
Teams, SAP, and ticketing systems to embed AI into daily
workflows.
Embed AI into operations through workflow integration,
automation triggers, and feedback loops.
7. AI – Human Factors 
Change resistance & skill
gaps
Investment in Adoption & Skills – Requests training,
documentation, and onboarding to drive user confidence and
sustained use.
Drive adoption and skills growth with change management, AI
champions, and continuous capability building.
8. AI Continuous Innovation 
Lack of momentum post-
launch
Early Vision of an Innovation Hub – Frames the AI Foundry as a
centralized center of excellence to coordinate innovation efforts.
Sustain innovation through governance, pipeline management,
MLOps, and strategic R&D.

Confidential | www.tha kra lone.co m
Aligning to our 8 Step Framework to Strengthen Project Outcomes
12
8 Step Framework Alignment Challenge Addressed Strengthening Project Success Potential Risk if Unaddressed
1. Enterprise AI Adoption Strategy 
Lack of clear AI vision &
executive alignment
Develop a strategic AI roadmap and executive alignment, Manila Water can
ensure initiatives are guided by a clear vision and linked directly to business
priorities.
Fragmented initiatives, misaligned investments,
and limited long-term impact. AI treated as a tool, not
a business driver.
2. AI Maturity Assessment & Ladder 
Unclear AI maturity &
readiness gaps
Incorporate AI maturity benchmarking, the project can map the “as-is” vs. “to-
be” states across people, technology, and processes, enabling phased,
sustainable scaling.
Over-engineering or under-scoping. Solutions
may not fit current organizational capabilities.
3. AI Technology & Data Foundations 
Data fragmentation &
quality issues
Assess enterprise-wide data readiness, standardization, and quality, system
integration efforts will be faster, more reliable, and future-proof.
Unreliable AI performance, bias in outputs, or
costly rework due to poor data hygiene.
4. AI Governance Framework 
Risk, bias, regulatory &
ethical concerns
Embed governance, ethics, privacy, and model accountability, Manila Water can
proactively manage Responsible AI and meet evolving regulatory standards.
Non-compliance, reputational damage, and
reduced stakeholder trust in AI systems.
5. AI Prototype Driven Use Cases Pilots failing to scale 
Define how PoCs will be validated, iterated, and scaled, the organization can
avoid “pilot purgatory” and ensure investments deliver lasting impact.
”Pilot purgatory” — high initial investment with no
business value realization.
6. Embedding Meaningful AI 
Integration & deployment
barriers
Plan for end-to-end integration and change enablement, AI solutions can
become seamlessly embedded into day-to-day operations.
Siloed or unused solutions. AI doesn't embed into
daily workflows or create real impact.
7. AI – Human Factors 
Change resistance & skill
gaps
Include structured user onboarding, change management, and workforce
upskilling, AI adoption can extend beyond IT to benefit the whole organization.
Low adoption, user pushback, and wasted
investment. Culture may resist or distrust AI.
8. AI Continuous Innovation 
Lack of momentum post-
launch
Establish future innovation structures like MLOps and an AI CoE, the AI
Foundry can remain a continuous driver of innovation well beyond project close.
Stagnation, loss of momentum, and inability to
scale AI efforts in the future.
By aligning fully to our 8-step framework, Manila Water can avoid problems associated with:
▪ Strategic Misalignment - Ensuring AI initiatives are unified under a clear, leadership-backed vision tied to measurable business priorities.
▪ Readiness Shortfalls - Deploy only when organizational maturity, skills, and infrastructure are ready to support scaling.
▪ Fragile Models – Build on strong data foundations and governance to prevent failures, errors, and compliance risks.
▪ Short-Term Focus – Establish a roadmap for integration, change management, and long-term operational sustainability.
▪ Missed Transformation – Embed AI as a core enterprise capability, not just a set of isolated tools.

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: 8 Step Framework Overview
13
Step Description Key Activities Typical Duration Deliverables
1. Enterprise AI
Adoption Strategy
Align AI with business goals through a Define where AI creates the most
business impact. Ensure CEO-led vision, set measurable goals, and
prioritize high-value use cases tied to strategic objectives.
Vision alignment, use case
discovery, strategic
prioritization
4 - 6 weeks 
AI Vision Brief, Use Case Portfolio,
Executive Alignment Plan
2. AI Maturity
Assessment & Ladder
Evaluate current capabilities across people, processes, data,
technology, governance & culture. Create a clear “as-is to to-be”
roadmap to close gaps and guide investment priorities.
Capability audit, gap
analysis, readiness
benchmarking
3 - 5 weeks 
Maturity Assessment Report, Roadmap &
Investment Priorities
3. AI Technology &
Data Foundations
Create a secure, scalable, AI-ready data environment. Break down
silos, modernize legacy systems, and establish systems for data quality,
metadata, and accessibility—essential for reliable models at enterprise
scale.
Data audit, infrastructure
review, tooling
recommendations
6 - 10 weeks 
Target Architecture, Data Readiness
Report, Integration Plan
4. AI Governance
Framework
Establish structures, policies, and safeguards to ensure responsible,
ethical, and compliant AI use. Build trust while enabling innovation at
scale.
Policy design, risk
framework, roles &
responsibilities setup
8 - 12 weeks 
AI Governance Charter, Risk Matrix,
Responsible AI Guidelines
5. AI Prototype Driven
Use Cases
Develop targeted AI prototypes to validate feasibility and impact. Use
early wins to build trust, involve end-users, and avoid disconnected or
stalled pilots.
Data ingestion, model
development, user testing,
feedback loops
4 – 8 weeks for each
protype (max. 12
weeks)
Working AI Prototypes, Pilot Impact
Report, Deployment Readiness Pack
6. Embedding
Meaningful AI
Move beyond pilot purgatory by scaling AI into daily operations. Ensure
integration into business workflows, supported by user training and
change readiness.
Workflow redesign,
training, change
enablement, rollout
planning
8 – 12 weeks 
Deployment Playbook, Change Plan,
Embedded AI Workflow
7. AI – Human Factors 
Address resistance and fears by engaging managers early, upskilling
staff, and fostering a human-centric culture that collaborates with AI.
Communication, capability
uplift, AI literacy programs 
4 – 8 weeks 
Training Modules, Engagement Materials,
Role Maps
8. AI Continuous
Innovation
Sustain momentum by embedding MLOps, scaling successful use
cases, and establishing an AI center of Excellence to drive long-term
value.
MLOps design, CoE
formation, innovation
tracking
Ongoing (initial 3–4
weeks setup)
CoE Charter, Innovation Pipeline, MLOps
Dashboard

Enterprise AI Adoption: Typical Engagement Timeline
▪ Step 1 begins immediately and finishes early to guide all subsequent work.
▪ Steps 2, 3, and 4 lay the groundwork in parallel with the early phases of prototyping.
▪ Steps 5 and 6 transition from pilot to production, overlapping with governance and culture work.
▪ Step 7 on change runs across most of the timeline and is critical for sustained adoption.
▪ Step 8 starts during late scaling to build momentum and sustainability.
Step 1: Enterprise AI Adoption Strategy
Step 2: AI Maturity Assessment & Ladder
Step 3: AI Technology & Data Foundation
Step 4: AI Governance Framework
Step 5: AI Prototype Drive Use Cases
Step 6: Embedding Meaningful AI
Step 7: AI – Human Factors
Step 8: AI Continuous Innovation
Step 1: Enterprise AI Adoption Strategy
Weeks 1–6
Align AI initiatives with business strategy to drive
measurable outcomes across the enterprise.
Step 2: AI Maturity Assessment & Ladder
Weeks 3-8
Evaluate current capabilities and set a clear
roadmap to scale AI effectively.
Step 3: AI Technology & Data Foundation
Weeks 3-14
Build scalable, secure data and infrastructure
foundations to support enterprise-grade AI.
Step 4: AI Governance Framework
Weeks 3-14
Establish ethical, risk-aware, and compliant
guardrails for responsible AI adoption.
Step 5: AI Prototype Driven Use Cases
Weeks 8–24
Deliver quick, value-focused AI pilots using real data
to prove business impact.
Step 6: Embedding Meaningful AI
Weeks 16–24
Integrate successful AI solutions into core
operations and enterprise systems.
Step 7: AI – Human Factors
Weeks 4-16
Equip teams with skills, tools, and change
management to ensure AI adoption sticks.
Step 8: AI Continuous Innovation
Weeks 24–Ongoing
Create a sustainable AI engine through MLOps,
CoE, and a pipeline of new use cases.
This roadmap delivers structured, accountable transformation—balancing early wins with long-
term capability building. It supports visible results by Week 10, scalable deployment by Week
16, and enterprise-wide readiness by Week 24+
W1 W4 W8 W12 W16 W20 W24 W32
Strategy 1-6
Foundations 2-14
Prototyping 8-20
Scaling 16-24
Continuity 24+
We tailor timelines based on organizational
readiness, regulatory complexity, and available
data infrastructure.
Confidential | www.tha kra lone.co m 
14

Our 8 Step Framework

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: Our 8 Step Framework
16
Step 3 establishes a robust
data & technology infrastructure
▪ Strong Foundation – Scaling AI
securely requires addressing data
silos, low data quality, and outdated
systems that block progress.
▪ Common Barriers – 70% of firms
face data governance issues, poor
integration, or insufficient data for AI
training.
▪ AI-Ready Stack – Build a scalable
data and tech stack (compute,
pipelines, cloud/on-prem tools)
aligned to compliance and security
standards.
▪ Outcome – Enable enterprise-wide AI
deployment using trusted, unified
data.
Step 2 assesses the
organisation’s current AI
maturity & outlines a path for
adoption
▪ Assess AI Maturity – Evaluate
people, processes, data & tech to map
where the enterprise stands on the “AI
maturity ladder” and identify scale-up
gaps.
▪ Multidimensional Model – Assess
maturity across areas like strategy,
data readiness, infrastructure,
organization & talent.
▪ Diagnostic Approach – Use surveys,
interviews, and reviews to pinpoint
gaps like missing skills or governance.
▪ Outcome – A clear “as-is vs. to-be”
map showing current level and next-
step targets to guide AI capability
uplift.
Step 4 establishes oversight
mechanisms to manage risks
associated with AI adoption
▪ AI Governance in Regulated
Industries – A robust framework is
vital to deploy AI responsibly and
safely at scale.
▪ Defined structures & policies –
Governance sets clear rules to
manage risks like bias, opacity,
security flaws, and regulatory
breaches.
▪ Critical for trust – Strong governance
builds trust and reduces risks that can
derail AI efforts.
▪ Outcome – A Fit-for-purpose model
aligned to regulations and ethics,
enabling innovation within safe limits.
Beyond the hype, how can I embed AI
and leverage benefits for my
enterprise?
What technology
foundations do I need?
How ready am I to adopt AI
strategically? 
How do I govern safe & effective AI?
Step 1 defines a comprehensive
AI vision & roadmap aligned with
the enterprise’s business strategy
▪ Strategic Focus – It establishes why
and where to invest in AI for maximum
business impact, ensuring executive-
level buy-in.
▪ CEO-Led Vision – AI success demands
top-down leadership; a clear narrative
avoids “AI for AI’s sake” and ensures
efforts are tied to real outcomes.
▪ Beyond Efficiency – High-impact
strategies explore what’s newly possible
with AI, not just automation of existing
tasks.
▪ Outcome – Sets measurable goals (e.g.
CX, operational gains) and prioritizes
high-value AI use cases that align to
them.
1 2 3 4

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: Our 8 Step Framework
17
Step 7 addresses the people
side of AI adoption by preparing
& enabling the workforce to
effectively work with & alongside
AI
▪ Cultural resistance and emotional
friction often pose greater barriers to AI
success than technical issues.
▪ Addressing human factors is crucial,
especially in sectors where employees
fear job loss or distrust AI decisions.
▪ This step includes change
management, training, role redefinition,
and building a culture of continuous
learning and human-AI collaboration.
▪ Outcome - to ensure AI augments, not
replaces, human capability, creating
confidence and synergy where staff
embrace AI tools and leadership
supports a human-centric approach.
Step 6 takes the successful
prototypes & embeds them into
core business processes at
scale
▪ Transitioning from experimentation to
real-world deployment means
embedding AI into daily operations, IT
systems, and workflows to deliver real
impact.
▪ This is often the toughest phase—
many firms get stuck in “pilot
purgatory,” where promising AI pilots
stall before full production.
▪ Studies show 70–90% of pilots fail to
scale.
▪ Outcome - to overcome those
barriers by tackling the core
challenges of scaling: integration,
performance, training, and change
management.
Step 8 establishes mechanisms
for continuous innovation &
improvement in AI within the
enterprise
▪ AI adoption is a journey, not a one-off
project. Technology and business needs
evolve—driven by new AI techniques,
regulations, and shifting priorities—so
organizations must adapt proactively.
▪ Continuous innovation embeds AI into
the company’s DNA—maintaining and
improving models (MLOps), exploring
new use cases, and scaling what works
across the business.
▪ AI Centers of Excellence (CoEs) help
centralize knowledge, avoid siloed efforts,
and keep the enterprise innovative while
maintaining strong AI governance.
▪ Outcome - to sustain momentum,
ensuring the enterprise doesn’t plateau
after early wins but expands AI’s value in
a strategic, controlled way.
How do I build investor confidence,
early?
How do I reduce resistance to
change & drive sustainable adoption?
How do I avoid overspend & generate
meaningful ROI?
How do I continuously improve & adapt
to rapid change?
Step 5 is about practical
experimentation: testing AI on a
small scale to validate feasibility
▪ Actionable use cases – Identify real
business problems and rapidly build AI
prototypes or PoCs to demonstrate value.
▪ Test at small scale – Validate feasibility
and impact before scaling to avoid
misaligned solutions disconnected from
actual needs.
▪ Use case criteria – Select use cases
based on business value and engage
end-users early for relevance and
usability.
▪ Prototyping impact – Turn abstract AI
ideas into tangible outcomes, driving
momentum and stakeholder confidence.
▪ Outcome – Deliver visible results and
build trust early with investors and
leadership.
5 6 7 8

Outcomes
▪ A clear AI strategy aligned with enterprise value.
▪ CXO sign-off on AI strategy with approved funding plan.
▪ Prioritized 6–10 use cases, with quick wins (<3 months ROI) and
strategic bets (>12 months) identified and costed.
▪ Enterprise AI as fixed board agenda update, with nominated
sponsor and reporting cadence agreed.
Confidential | www.tha kra lone.co m
Enterprise AI Adoption Strategy
18
Establish a clear, CEO-backed AI vision aligned to business goals (growth, efficiency, risk), ensuring AI is
treated as a strategic enterprise-wide initiative, not just technology experimentation.
Step 1 defines a
comprehensive AI vision &
roadmap aligned with the
enterprise’s business strategy
▪ Strategic Focus – It establishes why
and where to invest in AI for
maximum business impact, ensuring
executive-level buy-in.
▪ CEO-Led Vision – AI success
demands top-down leadership; a
clear narrative avoids “AI for AI’s
sake” and ensures efforts are tied to
real outcomes.
▪ Beyond Efficiency – High-impact
strategies explore what’s newly
possible with AI, not just automation
of existing tasks.
▪ Outcome – Sets measurable goals
(e.g. CX, operational gains) and
prioritizes high-value AI use cases
that align to them.
Activities
▪ Define AI vision linked to business outcomes.
▪ Conduct executive workshops to prioritize use cases.
▪ Benchmark against peers and industry AI trends.
▪ Develop a phased AI roadmap (12–24+ months).
▪ Set up an AI steering committee for governance & risk
alignment. Identify AI evangelists and boot up an AI
Think Tank to closely track strategic change and mood.
▪ Secure C-suite sponsorship, funding, and cultural
commitment.
Deliverables
▪ Enterprise AI Strategy & Operating Model Blueprint - Vision,
objectives, & business alignment.
▪ Use Case Portfolio – Prioritized AI opportunities with expected
value.
▪ Roadmap & Investment Plan – Sequenced execution, funding,
milestones.
▪ Initial AI Governance Charter – Oversight, ethics, and risk
framework.
▪ Executive Artefacts – CXO briefing deck, communication plan.
Resources & Estimated Effort
▪ Vendor: AI consultants, domain experts, data
strategists.
▪ Client: CEO, CIO/CDO, BU heads, strategy/innovation
team, finance, risk/compliance.
4–6 weeks of focused effort.
▪ Includes stakeholder interviews, CXO workshops, and
roadmap iteration.
▪ Can run parallel with early data/tech assessments.
Beyond the hype, how can I embed AI and
leverage benefits for my enterprise?
1

Outcomes
▪ Executive-level visibility on AI strengths, bottlenecks, and blockers
to scale.
▪ Actionable investment plan aligned to value acceleration and risk
mitigation.
▪ C-suite consensus on realistic AI scale-up path and ownership.
Confidential | www.tha kra lone.co m
AI Maturity Assessment & Ladder
19
Establish an objective baseline of the organization's AI readiness across people, processes, data, and
technology, defining “as-is” maturity & gaps to close for scale.
Step 2 assesses the
organization's current AI
maturity & outlines a path for
adoption
▪ Assess AI Maturity – Evaluate
people, processes, data & tech to
map where the enterprise stands on
the “AI maturity ladder” and identify
scale-up gaps.
▪ Multidimensional Model – Assess
maturity across areas like strategy,
data readiness, infrastructure,
organization & talent.
▪ Diagnostic Approach – Use
surveys, interviews, and reviews to
pinpoint gaps like missing skills or
governance.
▪ Outcome – A clear “as-is vs. to-be”
map showing current level and next-
step targets to guide AI capability
uplift.
Activities
▪ Adopt a proven or tailored AI maturity model
▪ Conduct stakeholder interviews, surveys, and artefact
reviews
▪ Evaluate across strategy, data, technology, talent, and
governance
▪ Benchmark against industry and peer organizations
▪ Synthesise current vs. target maturity level
▪ Identify gaps and improvement recommendations
Deliverables
▪ Maturity Assessment Report (scores, visuals, findings)
▪ Maturity Ladder Diagram (current vs. target level)
▪ Gap Analysis Matrix & roadmap recommendations
▪ Executive Readiness Briefing (prerequisites to proceed)
Resources & Estimated Effort
▪ Vendor: AI strategy consultants, assessors with sector
expertise
▪ Client: IT, analytics, risk, business leads, compliance,
strategy office
4–6 weeks
▪ Data collection (2–3 weeks) → Analysis & validation (2–
3 weeks)
How ready am I to adopt AI strategically?
2

Outcomes
▪ Enterprise-grade AI infrastructure
▪ Trusted, governed data sources
▪ Minimized integration & compliance risk
▪ “AI-ready” platform for rapid experimentation & scaling
Confidential | www.tha kra lone.co m
AI Technology & Data Foundations
20
Establish a secure, scalable, and compliant data and technology environment to support enterprise AI
experimentation and deployment.
Step 3 establishes a robust
data & technology infrastructure
▪ Strong Foundation – Scaling AI
securely requires addressing data
silos, low data quality, and outdated
systems that block progress.
▪ Common Barriers – 70% of firms
face data governance issues, poor
integration, or insufficient data for AI
training.
▪ AI-Ready Stack – Build a scalable
data and tech stack (compute,
pipelines, cloud/on-prem tools)
aligned to compliance and security
standards.
▪ Outcome – Enable enterprise-wide
AI deployment using trusted, unified
data.
Activities
▪ Data Architecture: Design scalable, governed data
lakes/lakehouses; unify siloed sources.
▪ Data Governance & Quality: Implement metadata, cataloguing,
access control, and data quality pipelines.
▪ Technology Stack: Select AI/ML platforms, MLOps tools, scalable
compute infrastructure (cloud/on-prem).
▪ Infrastructure Setup: Stand up dev/test/prod environments;
configure storage, networking, & security.
▪ Data Integration & APIs: Enable ingestion pipelines, real-
time/batch flows, and downstream connectivity.
▪ Baseline Tools: Deploy workbenches, sandboxes, and
collaboration environments for AI teams.
Deliverables
▪ Data & technology architecture blueprints
▪ Data governance policies, catalogue
▪ Data lineage & auditability
▪ Foundational infrastructure environments
▪ Compliance & security readiness checklists
▪ Initial AI platform/tool configurations
Resources & Estimated Effort
▪ Vendor: Data/Cloud architects, platform engineers, AI
solution architects
▪ Client: IT infrastructure, enterprise architects, data
owners, InfoSec & compliance
10–12 weeks
▪ Foundation setup runs parallel with initial use case
scoping
What technology foundations do I need?
3

Outcomes
▪ Formal safeguards for safe, ethical, and compliant AI
▪ Reduced regulatory and reputational risk mapped against relevant
& compulsory regulations & standards.
▪ Clear oversight, escalation, and accountability
▪ Enables safe innovation at scale with enterprise trust
Confidential | www.tha kra lone.co m
AI Governance Framework
21
Establish ethical, compliant & controlled oversight of AI systems—ensuring trust, safety, and regulatory
alignment at scale.
Step 4 establishes oversight
mechanisms to manage risks
associated with AI adoption
▪ AI Governance in Regulated
Industries – A robust framework is
vital to deploy AI responsibly and
safely at scale.
▪ Defined structures & policies –
Governance sets clear rules to
manage risks like bias, opacity,
security flaws, and regulatory
breaches.
▪ Critical for trust – Strong
governance builds trust and reduces
risks that can derail AI efforts.
▪ Outcome – A Fit-for-purpose model
aligned to regulations and ethics,
enabling innovation within safe limits.
Activities
▪ Form AI Governance Committee with defined roles &
escalation paths
▪ Draft enterprise AI policies on ethics, data use, model risk,
and bias testing
▪ Map regulatory requirements to governance controls
▪ Define AI use-case approval workflows and audit/reporting
protocols
▪ Roll out training & awareness for responsible AI practices
▪ Implement supporting tools (bias scanners, model cards,
drift monitors)
Deliverables
▪ AI Governance Charter & role structure
▪ Policy manual covering ethics, privacy, model validation
▪ Regulatory compliance mapping matrix
▪ Project approval workflow (e.g. RACI, review gates)
▪ Templates & toolkits for safe AI development (model docs,
checklists)
Resources & Estimated Effort
▪ Vendor: AI risk consultants, legal/regulatory advisors
▪ Client: Risk, Compliance, Legal, IT Security, Data
Science, Business Unit Leads
8–12 weeks
▪ Core policy development + review cycles (6 – 8 weeks)
▪ Training & rollout may extend parallel to pilots (2 – 4
weeks)
How do I govern safe & effective AI?
4

Outcomes
▪ Concrete validation of AI use cases
▪ Buy-in from stakeholders through hands-on engagement
▪ Process, data, & workflow insights for implementation
▪ Upskilled internal teams and “AI fluency” growth
▪ De-risked path to Step 6 (scaling)
Confidential | www.tha kra lone.co m
AI Prototype Drive Use Cases
22
Rapidly test high-potential AI use cases through targeted prototyping—validating value, feasibility, and
usability before broader investment.
Activities
▪ Identify & prioritize use cases (value, feasibility, alignment)
▪ Define business problem, success metrics, and data sources
▪ Develop AI prototypes (4–8 weeks) in agile, iterative sprints
▪ Involve end-users and domain experts for ongoing feedback
▪ Evaluate outcomes and decide: iterate, scale, or halt
▪ Capture lessons learned and update business case
assumptions
Deliverables
▪ Use Case Catalogue (prioritized, scoped, business-aligned)
▪ Working prototypes (models, dashboards, or apps)
▪ Pilot Results Report (KPIs, feedback, blockers, go/no-go)
▪ Updated Business Case for scaling
▪ Technical artifacts (code, datasets, model documentation)
Resources & Estimated Effort
▪ Vendor: AI engineers, data scientists, UX, analysts
▪ Client: Domain experts, data engineers, business
owners. Use a hub-and-spoke model for co-creation
with business units
4–8 weeks per prototype, multiple in parallel.
▪ Agile delivery with weekly demos & mid-point reviews
▪ Final showcase to execs to build momentum for scaling
How do I build investor confidence, early?
5
Step 5 is about practical
experimentation: testing AI on a
small scale to validate feasibility
▪ Actionable use cases – Identify real
business problems and rapidly build
AI prototypes or PoCs to demonstrate
value.
▪ Test at small scale – Validate
feasibility and impact before scaling
to avoid misaligned solutions
disconnected from actual needs.
▪ Use case criteria – Select use cases
based on business value and engage
end-users early for relevance and
usability.
▪ Prototyping impact – Turn abstract
AI ideas into tangible outcomes,
driving momentum and stakeholder
confidence.
▪ Outcome – Deliver visible results and
build trust early with investors and
leadership.

Outcomes
▪ AI operationalized and delivering real-world value
▪ Users adopt AI into standard workflows; trust and reliance
established
▪ organization builds repeatable blueprint for future AI deployments
▪ Tangible proof points to justify wider investment and scale-up
Confidential | www.tha kra lone.co m
Embedding Meaningful AI
23
Pilot to Production Operationalize AI prototypes into live, integrated systems with focus on model hardening,
IT integration, user adoption, & sustainable support to deliver real business value at scale.
Step 6 takes the successful
prototypes & embeds them into
core business processes at
scale
▪ Transitioning from experimentation to
real-world deployment means
embedding AI into daily operations, IT
systems, and workflows to deliver real
impact.
▪ This is often the toughest phase—
many firms get stuck in “pilot
purgatory,” where promising AI pilots
stall before full production.
▪ Studies show 70–90% of pilots fail to
scale.
▪ Outcome - to overcome those
barriers by tackling the core
challenges of scaling: integration,
performance, training, and change
management.
Activities
▪ Harden prototypes: production-ready code, model tuning,
scalability & integrate into IT: APIs, data pipelines, CRM/ERP
hooks
▪ Deploy with MLOps: CI/CD, containers, infra setup
▪ Validate at scale: UAT, performance, business KPIs,
governance
▪ Train users & manage change: onboarding, champions,
parallel runs
▪ Set up support: define ownership, helpdesk, monitoring
dashboards
▪ Measure impact: KPIs vs baseline, share success across org
Deliverables
▪ Live AI system (model, dashboard, API) integrated into
business processes
▪ Technical deployment documentation & support runbooks
▪ User manuals, training assets, knowledge base entries
▪ Monitoring dashboard and AI support ownership plan
▪ Go-live readiness report and production sign-off
▪ Internal case study capturing success metrics and lessons
learned
Resources & Estimated Effort
▪ Vendor: Software engineers, DevOps/cloud experts,
data scientists, UI/UX (if required), project manager
▪ Client: IT system owners, end-users, business SMEs,
compliance, operations/support staff
8–16 weeks per solution, (depending on complexity,
integrations, rollout scope).
▪ Concurrent activities (integration, training, change); 2–4
weeks post-launch stabilization period
How do I avoid overspend & generate
meaningful ROI?
6

Outcomes
▪ High adoption with AI becoming embedded in workflows &
measured against measurability and adoption criteria.
▪ Reduced resistance along with increase in trust and acceptance.
▪ Skilled workforce with employees adapting and growing with AI
against a defined % of roles within 6 months.
▪ Cultural alignment with organizational shifts toward data-driven, AI-
ready mindset.
Confidential | www.tha kra lone.co m
AI – Human Factors
24
Ensure successful AI integration by aligning people, roles & culture around new capabilities.
Step 7 addresses the people side
of AI adoption by preparing &
enabling the workforce to
effectively work with & alongside
AI
▪ Cultural resistance and emotional
friction often pose greater barriers to
AI success than technical issues.
▪ Addressing human factors is crucial,
especially in sectors where
employees fear job loss or distrust AI
decisions.
▪ This step includes change
management, training, role
redefinition, and building a culture of
continuous learning and human-AI
collaboration.
▪ Outcome - to ensure AI augments,
not replaces, human capability,
creating confidence and synergy
where staff embrace AI tools and
leadership supports a human-centric
approach.
Activities
▪ Align change strategy with AI rollout, ensuring leadership
visibly supports the vision and employee concerns are
addressed early.
▪ Engage staff through workshops & peer AI champions to build
trust, ownership, and reduce resistance.
▪ Deliver tiered training—from broad awareness to job-specific
skills and advanced technical upskilling—to build readiness.
▪ Redesign roles and KPIs where AI impacts workflows, with
HR-led reskilling plans for smooth workforce transitions.
▪ Promote a learning culture by sharing success stories and
capturing ongoing feedback through forums, surveys, and
peer support.
Deliverables
▪ Change Management Plan (stakeholder map, communication,
training, risk mitigation)
▪ Communication Materials (FAQs, leader memos, decks)
▪ Training Curriculum & Records (modules, materials,
participation tracking)
▪ HR Policy Updates (job descriptions, performance KPIs)
▪ Feedback & Adoption Metrics (usage logs, sentiment tracking)
▪ Workforce Transition Roadmap (reskilling, hiring plans)
Resources & Estimated Effort
▪ Vendor: Change consultants, HR specialists, AI trainers.
▪ Client: HR/L&D, Corporate Comms, IT leaders, line
managers, employee forums.
Ongoing, but major push ~4–8 weeks around AI go-live.
• Up-skilling may extend over months.
• Align with HR calendars (e.g. annual reviews,
performance cycles).
How do I reduce resistance to change &
drive sustainable adoption?
7

Outcomes
▪ Enterprise-level AI capability that evolves with technology &
business needs
▪ Continuous AI value generation across departments measured
against defined outcomes.
▪ Embedded AI culture, governance, & innovation mindset
Confidential | www.tha kra lone.co m
AI Continuous Innovation
25
Transform AI from a one-off project into an ongoing, enterprise-wide capability—driving repeatable innovation,
continuous value, and sustainable competitive advantage.
Step 8 establishes mechanisms
for continuous innovation &
improvement in AI within the
enterprise
▪ AI adoption is a journey, not a one-off
project. Technology and business
needs evolve—driven by new AI
techniques, regulations, and shifting
priorities—so organizations must adapt
proactively.
▪ Continuous innovation embeds AI into
the company’s DNA—maintaining and
improving models (MLOps), exploring
new use cases, and scaling what works
across the business.
▪ AI Centers of Excellence (CoEs) help
centralize knowledge, avoid siloed efforts,
and keep the enterprise innovative while
maintaining strong AI governance.
▪ Outcome - to sustain momentum,
ensuring the enterprise doesn’t plateau
after early wins but expands AI’s value
in a strategic, controlled way.
Activities
▪ Drive & Scale AI with an AI Centre of Excellence to govern
standards, run innovation pipelines, support business units,
and manage MLOps for model monitoring, retraining, and
decommissioning.
▪ Scout New Tech through R&D, partnerships, and hands-on
testing of emerging AI tools.
▪ Scale Proven Use Cases using templates, roadshows, and
cross-unit playbooks.
▪ Evolve Governance with updated policies, audits, and ethics
frameworks.
▪ Grow Talent & Culture via internal academies, career paths,
and continuous learning programs.
Deliverables
▪ AI CoE charter & engagement model
▪ MLOps dashboard & review schedule
▪ Innovation backlog (use cases & status)
▪ Internal R&D experiment notes
▪ AI asset library (code, APIs, docs)
▪ Talent dev programs (e.g. AI academy, hackathons)
Resources & Estimated Effort
▪ Vendor: Periodic advisory, emerging tech reviews,
managed services, or temporary staffing (if internal CoE
not yet mature).
▪ Client: AI CoE (data scientists, ML engineers, product
owners), exec sponsor, IT, risk, business unit liaisons.
Initial setup (CoE model, resourcing): 4–6 weeks.
• Ongoing Quarterly reviews, annual strategy refresh
embedded into BAU innovation rhythm with dedicated
resourcing
How do I continuously improve & adapt to
rapid change?
8

Architecture and Technical Delivery Approach

Confidential | www.tha kra lone.co m
Technical Delivery Approach Overview
27
Our approach integrates:
1. Architecture Governance: Formal design review
checkpoints, reference architectures, and technology
standards to validate key decisions before
implementation.
2. Agile Delivery: Scrum-based sprints aligned to our 8-
step framework, with sprint planning, daily stand-ups,
sprint reviews, and retrospectives to maintain pace and
quality.
3. DevSecOps Integration: Security and compliance
checks embedded into every CI/CD stage, ensuring that
infrastructure, code, and configurations meet enterprise
and regulatory requirements.
4. Automated Quality Gates: Linting, unit/integration tests,
security scans, and performance validations automatically
enforced before deployments.
5. Operational Handovers: Detailed runbooks, training
sessions, and knowledge-transfer workshops to transition
from project mode into sustainable operations. Includes
playbooks, admin guides, user-facing documentation, and
SME-led sessions
Thakral One’s Structured Technical Delivery Methodology ensures that every phase, from initial design through continuous
operations is governed by clear processes, defined artifacts, and automated pipelines.
Step AI Adoption Step Alignment to our 8 Step Framework
1 Enterprise AI Adoption Strategy 
Architecture governance checkpoints validate vision and design
artifacts before any build.
2 AI Maturity Assessment & Ladder 
Platform & Core Components build upon maturity findings to
establish the “To-Be” tech stack.
3 
AI Technology & Data
Foundation
Fabric Workspaces, OneLake medallion layers, and ingestion
pipelines directly implement foundational requirements.
4 AI Governance Framework 
DevSecOps scans, security pipelines, and audit logging enforce
policies defined in Step 4.
5 AI Prototype Driven Use Cases 
CI/CD pipelines and MLOps allow rapid sprint-based
deployments and automated validations for prototypes.
6 Embed Meaningful AI 
Blue-green/canary releases, runbooks, and monitoring
dashboards enable pilot-to-production transitions.
7 AI – Human Factors 
UI/UX standards, accessibility testing, and preview environments
support user adoption initiatives.
8 AI Continuous Innovation 
MLOps pipelines (drift detection, retraining) and CoE operational
processes institutionalize ongoing improvements.
These practices integrate with our feedback loop process to ensure continuous improvement during and after deployment.

Confidential | www.tha kra lone.co m
Implementation Roadmap & Methodology
28
Scope Boundary & Contracting Model
In-Scope (This Engagement)
▪ Consulting across all 8 steps including an Integration
▪ Readiness Assessment
▪ Build scope covers Phase 1 (Foundations) and Phase 2 (Core AI
MVP’s) only.
Future Options (Subject to approval)
▪ Phase 3 (Chat with Your Data, Predictive Models) and
▪ Phase 4 (Scale & CoE).
Commercials
▪ Base price covers Phases 1–2
▪ Phases 3–4 are not scoped or sized as part of the proposal.
Decision Gate
▪ Proceed to Phase 3 only if Phase-2 KPIs are met (e.g., HR
deflection ≥ 35%, ITSM TTR ↓ ≥ 20%, retrieval NDCG@10 ≥
0.8) and MWCI approval.
Implementation Roadmap: Phases 1 - 4
Our 4-Phase Roadmap translates the 8-Step AI Adoption Framework into an executable delivery plan, moving from secure
foundations to pilot delivery, scaled production, and institutionalized innovation. Each phase has clear deliverables, exit criteria,
owners, KPIs, and dependencies, ensuring transparency and control at every gate.
Phase 1 – Foundations
(Months 1-3)
▪ Fabric / OneLake
▪ MLFlow & DevSecOps
▪ Vector Store
Phase 2 – Core AI MVPs
(Months 4-6)
▪ HR Assistant
▪ ITSM Auto-Routing
▪ Doc Intelligence
Phase 3 – Advanced AI
(Future 7-9)
▪ Chat with Data (NL – SQL)
▪ Predictive Models
▪ Automation
Phase 4 – Scale & CoE
(Future 10+)
▪ Optimization & SLOs
▪ New Use Cases
▪ AI CoE & Handover

Confidential | www.tha kra lone.co m
Phase 1: Foundation & Custom Development Environment
29
Deliverables
▪ Three MVPs in staging
▪ Model cards
▪ Prompt libraries
▪ Power BI ops dashboard
▪ UAT packs & training guides.
Activities
▪ Fabric Data Science workspaces - MLflow registry; GitHub/ADO repos;
Terraform IaC; ACR/AKS baseline.
▪ Security foundations - Azure AD SSO/MFA; Key Vault; network isolation;
Purview catalogues & labels.
▪ External service integration patterns - Azure OpenAI, Azure ML endpoints;
usage & cost monitors.
▪ Custom vector store on Delta Lake - (embeddings pipeline + similarity
search); seed document index.
▪ Data ingestion to Bronze - quality checks to Silver; curated Gold tables for
first use cases.
Objectives
▪ Stand up secure Fabric/OneLake environments
▪ Wire up DevSecOps
▪ Enable governed access to SAP / ManageEngine / LIMS / SCADA
▪ Prepare for rapid prototyping.
Exit / Phase Gate Criteria
▪ Pen-test & policy scans: 0 Critical/High; keys/identities in Key Vault;
RBAC/ABAC verified.
▪ Data-quality thresholds met for target domains (completeness ≥ 98%,
schema drift = 0 critical).
▪ CI/CD green (unit, SAST, DAST smoke); UAT sign-off for foundations by
MWCI Product Owner.
KPIs
▪ Time-to-provision environments (<2 weeks)
▪ Pipeline success rate (>99%)
▪ Data freshness SLAs met.
Dependencies
▪ ASAPIO connectivity to SAP
▪ ManageEngine API access
▪ SCADA/LIMS read endpoints
▪ MWCI identity groups.
Phase 1 – Foundations
(Months 1-3)
▪ Fabric / OneLake
▪ MLFlow & DevSecOps
▪ Vector Store

Confidential | www.tha kra lone.co m
Phase 2: Core AI Development
30
Deliverables
▪ Environment Runbook
▪ Security Baseline Report
▪ IaC repositories
▪ MLflow registry live
▪ Purview data map
▪ Baseline vector index + RAG utilities
▪ Initial Bronze/Silver/Gold tables
▪ CI/CD pipelines (build/test/deploy).
Activities
▪ HR AI Assistant: Conversational UX; intent/entity models; SuccessFactors
integration via ASAPIO; audit logging.
▪ ITSM Automation: Ticket classification/priority models (SynapseML); auto-
routing; blended chat; ManageEngine flows.
▪ Document Intelligence: Parsing/summarization; vector search; knowledge
cards with source citations.
▪ UX & Accessibility: WCAG 2.1 AA; English/Tagalog i18n; Storybook +
Cypress tests.
Objectives
▪ Deliver working MVPs for HR Assistant
▪ ITSM auto-routing
▪ Document Intelligence with measurable business impact.
Exit / Phase Gate Criteria
▪ HR intent accuracy ≥ 90% on UAT set
▪ ITSM routing F1 ≥ 0.85
▪ doc retrieval NDCG@10 ≥ 0.8.
▪ p95 API latency < 200 ms; no Critical/High vulnerabilities
▪ stakeholder UAT sign-off.
KPIs
▪ HR deflection rate (≥ 35% of common queries)
▪ IT ticket TTR reduction (≥ 20%)
▪ Search success (> 85%).
Dependencies
▪ Access to operational telemetry
▪ Finalization of semantic layer
▪ Business KPI definitions
▪ HR Assistant
▪ ITSM Auto-Routing
▪ Doc Intelligence
Phase 2 – Core AI MVPs
(Months 4-6)

Confidential | www.tha kra lone.co m
Phase 3: Advanced AI Capabilities
31
Deliverables
▪ NL→SQL service in staging
▪ 2 predictive models in staging
▪ Automation playbooks
▪ Runbooks
▪ DR procedures.
Activities
▪ NL→SQL service with guardrails
▪ SQL explainability, Power BI integration, semantic dictionary of MWCI terms.
▪ Predictive maintenance/demand forecasting models
▪ Scheduled scoring + feedback loops.
▪ Data Activator/Logic Apps workflows for proactive actions & approvals.
▪ Canary releases, blue/green for critical endpoints
▪ Production dashboards & alerts.
Objectives
▪ Launch "Chat with Your Data" (NL→SQL + RAG)
▪ First predictive models
▪ Embed event-driven automations..
Exit / Phase Gate Criteria
▪ NL→SQL query success rate ≥ 80% (first attempt) with safe-guarded
fallbacks.
▪ Predictive models exceed baseline by ≥ 10–15% on target KPIs; DR test
pass; on-call runbook approved.
KPIs
▪ Time-to-insight (< 60s from question to chart)
▪ Incident auto-triage coverage (≥ 25%)
▪ Forecast MAPE improvement (≥ 15%).
Dependencies
▪ Access to operational telemetry; finalization of semantic layer; business KPI
definitions.
Phase 3 – Advanced AI
(Future 7-9)
▪ Chat with Data (NL – SQL)
▪ Predictive Models
▪ Automation

Confidential | www.tha kra lone.co m
Phase 4: Scale & Standing up the CoE
32
Deliverables
▪ Performance Optimization Report
▪ Updated SLO/SLA
▪ CoE Charter & engagement model
▪ knowledge base & KT packs.
Activities
▪ Performance tuning (compute sizing, caching, vector search efficiency); cost
optimization.
▪ Expand to additional high-value use cases; finalise governance workflows
and model risk tiers.
▪ Establish AI CoE: operating model, backlog intake, reuse templates, quarterly
roadmap.
▪ Training programs (exec, power user, admin); hypercare & support model
activation.
Objectives
▪ Harden for scale
▪ Expand use cases
▪ Stand up CoE and continuous improvement engine.
Exit / Phase Gate Criteria
▪ Availability ≥ 99.9%
▪ Cost-to-serve within budget
▪ CoE operational
▪ Handover accepted
▪ Hypercare complete.
KPIs
▪ User adoption (MAU growth)
▪ Production incident MTTR (< 1h Sev2)
▪ Model drift MTTA (< 24h).
Dependencies
▪ Budget approvals for scale
▪ Staffing for CoE
▪ Finalized support SLAs.
Phase 4 – Scale & CoE
(Future 10+)
▪ Optimization & SLOs
▪ New Use Cases
▪ AI CoE & Handover

Confidential | www.tha kra lone.co m
Solution Blueprints: HR Operations AI (Assistant, Onboarding, Leave)
33
Scope & Value
Conversational HR assistant for policies, benefits, leave and onboarding;
reduces HR ticket load and speeds up employee resolution.
CI/CD & MLOps Hooks
▪ Intent/entity models tracked in Mlflow
▪ Unit tests for state store
▪ Contract tests for ASAPIO/Teams
▪ Canary releases for new prompts/models
▪ Drift monitoring on intents
▪ Rollback on KPI regression.
These high-level blueprints show how we will implement the highest-priority capabilities from the TOR during Phases 1–2.
Each blueprint specifies data lineage, interfaces, CI/CD and MLOps controls, security guardrails, and measurable KPIs.
Architecture
▪ RAG pattern: retrieve policy snippets from vector store (Delta in OneLake),
ground an Azure OpenAI prompt, return answer with citations;
▪ Conversation state persisted in Delta tables for continuity and audit; actions
(e.g., leave request) posted to SAP via ASAPIO connectors;
▪ Guardrails: approved prompt templates, content filters, PII redaction,
profanity/bias checks.
Data & Integrations
▪ SAP SuccessFactors via ASAPIO → OneLake Bronze (raw) → Silver
(standardised HR entities);
▪ Policy docs (SharePoint/OneDrive) → document pipeline → embeddings in
OneLake;
▪ Microsoft Teams channel for chat UI; Azure AD for identity & RBAC; audit
logs in Delta tables.
Security & Governance
▪ PII masking in training sets; ABAC on HR tables
▪ HITL review for sensitive topics (payroll, disciplinary).
KPIs / NFRs
▪ Intent F1 ≥ 0.90, deflection ≥ 35%
▪ p95 latency < 200 ms
▪ CSAT ≥ 4.2/5
▪ Availability ≥ 99.9%.
Click here for the POC

Confidential | www.tha kra lone.co m
Solution Blueprints: ITSM AI (Classification, Predictive, Automation)
34
Scope & Value
• Auto-classify, route and prioritize tickets
• Predict SLA risks
• Trigger automations that reduce MTTR and improve FCR.
CI/CD & MLOps Hooks
▪ PySpark pipelines with unit/integration tests
▪ MLflow model registry with bias/explainability gates
▪ Canary on 10% traffic
Models & Automation
▪ Text classification for category/priority/assignment
▪ Reopen detection
▪ SLA-breach prediction;
▪ Logic Apps / Power Automate flows to open, update, escalate or close
tickets with human approvals.
Data & Integrations
▪ ManageEngine ServiceDesk tickets → OneLake Bronze/Silver
▪ Knowledge base docs → embeddings
▪ Event hooks via webhooks/Event Grid.
Security & Governance
▪ Least-privilege API accounts
▪ full audit trail of automated actions
▪ change windows and kill-switch.
KPIs / NFRs
▪ Routing F1 ≥ 0.85
▪ SLA breaches ↓ ≥ 20%
▪ MTTR ↓ ≥ 20%
▪ p95 inference < 150 ms.
Click here for the POC

Confidential | www.tha kra lone.co m
Solution Blueprints: Knowledge Management (Doc Intel, GenAI, Semantic Search)
35
Scope & Value
Central knowledge discovery with semantic search and grounded
generation to accelerate answers and reduce duplicate tickets..
CI/CD & MLOps Hooks
▪ Regression tests on retrieval quality (NDCG@10)
▪ Hallucination tests on held-out corpus
▪ Prompt packs versioned in Git.
Architecture & UX
▪ Document pipeline (parse → clean → embed)
▪ RAG answering with source citations and confidence
▪ UI flags low-confidence answers for human review.
Data & Integrations
▪ PDFs/Docs/Emails from SharePoint/Exchange → extraction → chunking →
embeddings → Delta vector tables in OneLake;
▪ Optional Azure Cognitive Search vector indexes for scale/ops simplicity
(switch when query TPS or admin overhead exceeds internal thresholds).
Security & Governance
▪ Access-controlled collections;
▪ Redaction at ingestion
▪ Lineage in Purview
▪ Retention policies enforced.
KPIs / NFRs
▪ NDCG@10 ≥ 0.80
▪ Citation coverage ≥ 95%
▪ Hallucination rate < 2%
▪ P95 search < 1.0 s.
Click here for the POC

Confidential | www.tha kra lone.co m
Platform & Core Components: Overview
36
Governance, Security &
 
MLOps
 
Layer
▪
 
Azure AD (SSO, RBAC, ABAC)
▪
 
MLflow
 
(Model Registry)
▪
 
Microsoft Purview
▪
 
Azure Sentinel / Key Vault
Thakral One recommends the core
AI Foundry platform is built on
Microsoft Azure and Fabric services,
integrated with Manila Water’s
enterprise systems.
This foundation supports high availability, multi-region
deployment, and continuous improvement through
built-in MLOps and feedback loops.
It supports conversational, generative, and predictive
AI through modular, scalable components.
▪ Seamless integration of generative and conversational AI
▪ Natural language access to Lakehouse data
▪ End-to-end governance and secure model operations
USER INTERFACES
▪ Web UI (React) / MS Teams / Power BI
AI & ORCHESTRATION LAYER
▪ Agent AI (HR, ITSM)
▪ Generative AI (LLMs via Azure OpenAI)
▪ RAG-based 'Chat with Your Data'
DATA & INTEGRATION LAYER
▪ Microsoft Fabric (OneLake)
▪ Delta Lake (Bronze, Silver, Gold)
▪ SAP SuccessFactors / ManageEngine / SCADA

Confidential | www.tha kra lone.co m
Platform & Core Components
37
Layer Component Purpose & Capabilities
Compute & Orchestration 
▪ Fabric Data; Science Workspaces
▪ Azure Kubernetes Service (AKS)
▪ Isolated, scalable Spark clusters for model training.
▪ Container orchestration for inference engines and microservices
▪ Auto-scaling based on workload demands
Storage & Data Lake 
▪ OneLake (Delta Lake)
▪ Azure Data Lake Gen2
▪ Medallion architecture managing raw, curated, and serving data
▪ Native support for ACID transactions, time travel, and schema enforcement
Model Lifecycle 
▪ Azure Machine Learning
▪ MLflow on Fabric
▪ Centralised model registry with versioning and lineage
▪ Automated training pipelines and experiment tracking
▪ Support for custom Python, PySpark, and R frameworks
▪ Includes model governance workflows for auditability and approval
AI Services
▪ Azure Open AI
▪ Service SynapseML
▪ Evidently.ai
▪ Generative LLMs for text generation, summarization, and chat
▪ Distributed ML algorithms for large-scale training
▪ Data drift and model monitoring with automated alerts
Integration & API
▪ Azure API Gateway / Management
▪ Logic Apps
▪ Event Grid
▪ Unified API gateway with policies, rate-limiting, and security
▪ Orchestration of event-driven workflows (real-time & batch)
▪ Support for large-scale ingestion and real-time triggers from SAP, LIMS, SCADA, etc.
Security & Governance
▪ Azure AD (RBAC/ABAC)
▪ Microsoft Purview
▪ Key Vault
▪ Fine-grained access control and single sign-on
▪ Data catalog, classification, lineage, and policy enforcement
▪ Secure secret management with automated key rotation
▪ Support for audit trails, consent tracking, and compliance dashboards
DevSecOps & CI/CD
▪ GitHub Actions
▪ Azure DevOps
▪ Pipelines & Terraform
▪ Infrastructure-as-code for reproducible environments
▪ Automated build, test, and deployment workflows with security scans
▪ Environment drift detection and compliance checks
Observability & Ops
▪ Azure Monitor
▪ Application Insights
▪ Log Analytics
▪ End-to-end telemetry for pipelines, models, and UI
▪ Custom dashboards for performance, usage, and cost metrics
▪ Alerting and on-call integrations for incident response
▪ Model performance tracking

Confidential | www.tha kra lone.co m
Key Architectural Patterns
38
1. Medallion Architecture for Data Refinement
▪ Bronze Layer: Ingest raw data from SAP, ManageEngine, SCADA, LIMS into
Delta Lake with schema-on-read.
▪ Silver Layer: Cleanse, standardise, enrich datasets via PySpark transforms and
Azure Data Factory orchestration. Apply data quality checks (null detection,
schema validation) and lineage tracking with Purview.
▪ Gold Layer: Curated, analytics- and model-ready tables. Create aggregated
feature tables and summary views to boost ML pipelines and dashboards.
2. Containerised Model Serving & Microservices
▪ Models packaged as Docker containers with dependencies.
▪ Deployed on Azure Kubernetes Service (AKS) with horizontal pod autoscaling
for variable inference loads.
▪ Deployed from MLflow registry, exposed via secure APIs with RBAC via Azure
AD.
▪ Routed through Azure API Management for rate-limiting, authentication, and
canary releases.
3. Event-Driven & Stream Processing
▪ Use Azure Event Grid and Service Bus to capture system events (e.g., SAP
updates, ticket creation, sensors) in real-time.
▪ Fallback batch polling if event triggers fail.
▪ Azure Functions or Logic Apps trigger micro-batch Spark jobs or serverless
functions to refresh real-time pipelines, keeping AI services like “Chat with Your
Data” responsive.
These architectural patterns ensure MWC’s AI Foundry is built for reliability, scalability, and maintainability, crucial for
enterprise-grade deployments and ongoing innovation.
4. Feature Store Integration & Reusability
▪ Centralised Feature Store on Delta Lake stores engineered features with version
metadata.
▪ Training/inference pipelines read from the Feature Store for consistency.
▪ Supports online retrieval via REST API for low-latency conversational/predictive
scoring.
▪ Reuses HR/ITSM features (e.g., leave types, issue categories) across
classification, summarization, and auto-routing.
5. Infrastructure-as-Code & Immutable Environments
▪ Manage all infra/config via Terraform for reproducibility, version control, drift
prevention.
▪ Immutable deployments: changes trigger new environment provisioning and
tear-down of old, simplifying rollback.
▪ Validate infra changes with policy-as-code to detect misconfigurations and
enforce compliance with security standards.
6. Observability & Automated Remediation
▪ Combine Azure Monitor, Application Insights, Log Analytics to track metrics for
pipelines, models, APIs, UI.
▪ Automated remediation via Azure Automation runbooks: e.g., on ingestion
failure, trigger alert, fallback to last snapshot, and create ManageEngine ticket
for review.
▪ Metrics feed loops driving model improvement, retraining, and SLA assurance.

Confidential | www.tha kra lone.co m
Our Recommendation: Why Fabric + Azure AI & not only Fabric
39
What Fabric is best at (data plane)
▪ Enterprise lakehouse (OneLake, medallion architecture),
pipelines, notebooks, Power BI, and tight governance via
Purview.
▪ Collaborative dev in Fabric Data Science workspaces
with MLflow for experiments/registry.
▪ Running prototypes and analysis close to the curated
enterprise data.
Security & compliance fit
▪ Single-tenant Azure AD auth across both planes, Key Vault
for secrets, Purview for lineage, and data residency in MWCI-
approved regions.
▪ Easier auditability: Fabric holds the governed data; Azure AI
holds the model endpoints—clean separation of duties.
What Azure AI adds (model plane)
▪ Azure Machine Learning for production-grade
MLOps: managed online/batch endpoints, canary/blue-
green, autoscaling, A/B tests, model registries across
workspaces, GPU scheduling, and fine-tuning jobs.
▪ Azure OpenAI for enterprise LLMs with tenant
controls, content safety, data-use protections, and
regional residency.
▪ AKS/Container Apps for low-latency, high-QPS
inference and private networking.
▪ Azure AI Search (vector) when QPS/ops exceed what
a Delta-based vector store can comfortably handle.
Operational reliability & cost
▪ Fabric handles data engineering and analytics
▪ Azure ML/AKS handles elastic inference with autoscale
policies → you pay for capacity only when used.
▪ Decoupling training (Fabric/Azure ML) from serving (Azure
ML/AKS) reduces blast radius and simplifies rollback.

Confidential | www.tha kra lone.co m
End to End CI/CD Pipeline Overview
40
Our CI/CD framework follows a shift-left DevSecOps philosophy, integrating
security, quality, and compliance as early as possible. It encompasses five
stages to ensure that all code and infrastructure changes are automated,
tested, and governed before reaching production.
CI/CD Framework Stages
1. Plan: Requirements and code are managed via GitFlow, with issues and backlog tracked in
Azure Boards for traceability.
2. Build: Pipelines build Docker images and infrastructure modules (e.g. Terraform), with static
analysis for security and syntax validation.
3. Validate: Unit, integration, and security tests are executed; containers are scanned for
vulnerabilities and test coverage reported. Promotion gated by role -based approvals via
Azure DevOps environments and service connections.
4. Release: Blue/green or canary deployments orchestrated via Azure DevOps deploy artifacts
to staging, run smoke tests, and promote to production upon success.
5. Monitor: Post-deployment health checks, performance monitoring, and automated rollback
policies maintain service reliability. Includes alerting and uptime SLAs for core services like
‘Chat with Your Data’ and HR/ITSM assistants.
Example GitHub Actions Snippet

Confidential | www.tha kra lone.co m
Source Control & Branching
41
Source Control & Branching
• GitHub Enterprise with GitFlow for feature isolation.
• Protected Branches for main, develop, and release branches.
Continuous Deployment (Azure DevOps
Pipelines)
▪ Infrastructure-as-Code: Terraform modules managing
Fabric workspaces, Key Vaults, and networking.
▪ Blue-Green Deployments: Azure ML endpoints deployed in
staging, tested via smoke tests, then promoted to
production.
▪ Canary Releases: UI updates deployed to 10% user
segment; monitored via Application Insights before full
rollout.
▪ Automated smoke tests or SLO thresholds before traffic is
allowed in production.
Artefact Packaging & Registry
▪ Model Packages: Built into Docker containers; versioned and stored in
Azure Container Registry (ACR).
▪ UI Assets: Bundled with Webpack; artifacts published to Azure Storage for
Static Web Apps.
Continuous Integration (GitHub Actions)
▪ Linting & Static Code Analysis: pylint, flake8 for Python; eslint for
TypeScript; tflint for Terraform.
▪ Unit Tests: pytest for Python modules; jest for React components; Spark
unit tests using pytest-spark. Coverage thresholds enforced.
▪ Integration Tests: Post-deploy contract tests via Postman collections;
Spark end-to-end dataflow tests using sample Delta tables.
▪ Security Scans: SAST via bandit for Python; dependency scanning via
GitHub Dependabot; container scanning with Trivy. Container scan failures
block release. 
Automated Validation & Monitoring
▪ Smoke Tests: Post-deploy validation against health
endpoints.
▪ Performance Tests: Load testing via Locust for APIs and
conversational interfaces.
▪ Rollback Policies: Failure triggers automatic rollback to
last known good deployment.

Confidential | www.tha kra lone.co m
User Interface & Experience
42
Performance Optimization
Code splitting with React.lazy, optimised bundle sizes via dynamic
imports, and image/svg inlining. Pre-fetch critical routes and assets to
achieve <1s Time-to-Interactive.
Responsive & Adaptive Layouts
Mobile-first design with Flexbox and CSS Grid to support desktop,
tablet, and mobile form factors. Breakpoints and adaptive
behaviors optimise readability and interaction on varying screen
sizes.
Our UI/UX approach combines design thinking, accessibility, and performance optimization to deliver intuitive, responsive, and
enterprise-grade interfaces tuned for MWCI users.
State Management & Data Flow
React Context and Redux Toolkit (where complex) manage
application state, ensuring predictable data flow between UI widgets,
API layers, and local storage - even during latency or offline
scenarios (e.g., large file processing, slow networks) – ie retaining
user context when chatting with an HR bot.
Design System & Component Library
Consistent enterprise UI built using modern (centralized) component
libraries (shadcn/ui, Tailwind), ensuring accessible, brand-aligned visual
standards across all modules. Features design tokens for color,
typography, spacing, and interactive states. Ensures visual consistency
across HR assistants, ticketing consoles, and data explorers.

Confidential | www.tha kra lone.co m
Core UI Modules
43
Conversational Chat Interface
• Real-time chat assistant supporting HR and ITSM queries, built for
responsiveness and usability. Features include predictive suggestions,
multi-step interactions, and context memory for seamless employee
support.
• Built with React hooks and WebSockets for real-time messaging.
Adaptive UX: Progressive disclosure of commands with quick-reply
buttons for common queries. Session Persistence: Local Indexed DB
storage synchronises with OneLake, preserving chat context across
sessions.
Admin & Monitoring Dashboard
• Real-time Health Indicators: Cards displaying API uptime,
model latency, and message throughput.
• Alert Management: Inline alert list from Application Insights,
with links to logs and rerun options.
• User Management: Role-based access controls surfaced via
Azure AD groups, with UI to assign permissions and audit user
actions.
Data Query Explorer
• Enables employees to query enterprise data in the Lakehouse
using natural language. Returns structured results, generates
SQL, and integrates with Power BI dashboards. Includes semantic
search and document previews for deep knowledge access.
• Natural Language to SQL; Power BI Embeds; Semantic Search
Panel: Vector search results ranked by relevance, with document
previews and drill-down links.
Prompt Engineering Console
• Self-service prompt console enabling users to test, refine, and version
prompts for generative AI agents. Includes real-time response previews,
usage tracking, and model comparison tools for business
experimentation.
• Live Preview: 3-panel layout showing prompt input, LLM response, and
token usage metrics.
• Version Control: Integrated with GitHub to snapshot prompt revisions and
allow rollback.
• Experimentation Mode: Toggle between multiple models (e.g. GPT-4,
custom fine-tuned models) with A/B side-by-side comparison.

Confidential | www.tha kra lone.co m
Accessibility & Internationalization
44
WCAG 2.1 AA Compliance
Semantic HTML, ARIA roles, keyboard navigability, focus indicators, and high-
contrast themes validated via axe-core and manual audits.
Internationalization (i18n)
(react-i18next) integration with locale detection, fallback
language support, and dynamic resource loading. Support for:
▪ Date/Time & Numeric Formats: Locale-aware formatting
(e.g., dd/mm/yyyy vs mm/dd/yyyy, decimal separators).
▪ Currency & Units: Automatic conversion and formatting
based on user locale settings.
▪ Text Expansion: UI layouts tested to accommodate
languages requiring more space (e.g., Tagalog, potentially
longer translations).
Ensuring inclusivity and broad usability is essential for maximum adoption—our interfaces are designed to be accessible for all
employees and adaptable to local language preferences, which enhances satisfaction and productivity.
Color Contrast & Visual Design
Automated contrast checks (WCAG ratio >=4.5:1), support for custom high-
contrast mode, and color-blind friendly palettes.
Screen Reader & Keyboard Support
Ensure all interactive elements (buttons, forms, chat controls) are operable via
keyboard and announce clear labels to assistive technologies.
Translation Workflow
Integration with a translation management system (e.g.,
Crowdin) enabling continuous localization; clear context keys
and developer in-context comments for translators.

Confidential | www.tha kra lone.co m
End to End UI Testing & Quality
45
Component-Level Testing
Each UI widget is validated in Storybook with unit tests (using Jest) and visual
regression checks (via Chromatic) to catch styling or behavioral changes early.
Cross-Browser & Device Testing
Use BrowserStack automated tests to verify UI functionality on
supported browsers (Chrome, Edge, Safari) and devices
(desktop, tablet, mobile).
For a seamless and reliable user experience, we validate every element from individual components to full user journeys. This
ensures that MWCI users receive performant, accessible, and consistent interfaces in production.
Accessibility Audits
Regular runs of axe-core as part of Cypress flows to validate WCAG
compliance across pages, capturing violations in CI reports.
Integration Testing
Automated Cypress suites simulate key user journeys (login, chat interactions,
data query, prompt editing) against staging environments; tests run on every
pull request and nightly.
Usability Testing
Periodic moderated sessions with MWCI users to gather UX
feedback; findings logged in Azure DevOps and prioritized in
sprints.
Release Validation
Smoke tests run post-deployment to ensure core UI
functions (chat, query explorer, dashboards) are operational
before full rollout.

Confidential | www.tha kra lone.co m
Data and Model Operations (MLOps)
46
Tools & Technologies Key Activities & Controls
Data Ingestion & Prep Azure Data Factory, PySpark, Delta Lake
▪ Ingest raw data into Bronze layer with schema-on-read
▪ Apply cleansing, standardization, and enrichment jobs to Silver layer
▪ Validate data quality metrics (completeness, accuracy) via automated tests
Feature Engineering 
Delta-based Feature Store, Databricks
(optional)
▪ Define reusable feature tables with versioned metadata
▪ Automate feature computation pipelines triggered by data refresh
▪ Store feature definitions and lineage in Purview for auditability
Model Training & Tuning Azure ML Pipelines, MLflow, HyperDrive
▪ Orchestrate end-to-end pipelines: data prep, training, evaluation, registration
▪ Track experiments and parameters in Mlflow
▪ Perform hyperparameter optimization via HyperDrive or SynapseML AutoML
Validation & Explainability MLflow, SHAP, LIME, Evidently.ai
▪ Validate model performance on holdout sets; generate metrics dashboards
▪ Conduct bias and fairness tests across demographic segments
▪ Generate explainability reports (SHAP, LIME) and store model cards in registry
Deployment & Serving 
AKS, Azure Container Instances, Azure ML
Endpoints
▪ Package models into Docker containers with dependencies
▪ Deploy to staging endpoints using blue/green or canary strategies
▪ Expose inference APIs via API Management with rate-limits and auth checks
Monitoring & Drift Evidently.ai, Application Insights, Azure Monitor 
▪ Monitor model inputs and outputs for data drift, concept drift, and performance decay
▪ Configure alerts for drift thresholds and operational anomalies
A robust MLOps practice is critical for maintaining model reliability, reproducibility, and performance at scale. By automating
data pipelines, training, validation, deployment, and monitoring, we minimise manual errors and accelerate model iteration.

Confidential | www.tha kra lone.co m
Sample Automated Retaining Workflow
47
2. Trigger Retrain
If KS > 0.1, DF invokes an Azure ML
Pipeline with a “RebuildModel” step.
5. Canary Deployment
Approved models are deployed behind API
Management to 10% of production traffic for 24 hours.
4. Governance Gate
For high-risk use cases, an MLflow UI card appears
for the AI Governance Lead to approve or reject
All results and decisions are logged for audit.
1. Drift Detection (2 AM Daily)
Data Factory runs a PySpark job that
computes KS-statistic on key feature
distributions via Evidently.ai.
3. Automated Validation
Pipeline runs unit, integration, and regression tests.
It writes a “Pass/Fail” JSON report to OneLake.
6. Monitoring & Promote
Application Insights compares live metrics. If
within 5% of SLA, automated promotion to
100%; else rollback.

Confidential | www.tha kra lone.co m
Security & Compliance Controls (1/2)
48
Network & Infrastructure Security
▪ Zero-Trust Network Architecture: All compute and storage endpoints reside in
private VNets with NSGs, Azure Firewall, and service endpoints.
▪ Segmentation & Micro-Segmentation: Separate subnets for dev/test/prod
environments, with granular traffic rules.
▪ DDoS Protection & WAF: Azure DDoS Standard and Web Application Firewall
guard against volumetric and application-layer attacks.
Identity & Access Management
▪ Azure Active Directory Integration: Enforce single sign-on (SSO) and MFA for all
users and service principals.
▪ Role-Based (RBAC) & Attribute-Based Access Control (ABAC): Fine-grained
Azure RBAC roles for workspaces, Key Vault, and storage; ABAC policies for data
sensitivity labels via Purview.
▪ Just-In-Time (JIT) Access: Temporary elevation of privileges for admin tasks
through Privileged Identity Management (PIM).
▪ All user/model/API interactions are logged with trace IDs to ensure full auditability
and forensics.
Data Protection & Privacy
▪ Encryption: AES-256 encryption at rest in
OneLake and in transit via TLS1.2+.
▪ Data Masking & Tokenization: Dynamic
data masking and tokenization for PII
fields using Azure SQL/managed
databases.
▪ Data Classification & Governance:
Automated discovery and labeling in
Microsoft Purview; policies enforce data-
handling rules (e.g. no export of masked
data).
▪ Consent & Privacy Controls: Integration
with consent management platform to
track user permissions for data use, with
audit trails stored in Purview.
Securing MWC’s AI Foundry requires a defense-in-depth approach, embedding security and compliance at every layer: network,
platform, data, and application, to protect sensitive data, meet regulatory obligations, and maintain operational continuity.
1 
3
2

Confidential | www.tha kra lone.co m
Security & Compliance Controls (2/2)
49
DevSecOps & Secure Development
▪ Infrastructure-as-Code Scanning: Policies enforced in Terraform with Checkov to
detect misconfigurations.
▪ Secure Coding Practices: SAST (Bandit, ESLint) and DAST (OWASP ZAP)
embedded in CI pipelines.
▪ Container Security: Image scanning with Trivy and Azure Defender for
Containers.
Application & Model Governance
▪ Azure Active Directory Integration: Enforce single sign-on (SSO) and MFA for all
Model Risk Assessment: Automated bias and fairness checks using SHAP and
LIME; results logged in MLflow.
▪ Approval Workflows: MLflow gates require sign-off from AI Risk & Compliance
Lead before high-risk models promote to production.
▪ Explainability & Audit Trails: Generate model cards and lineage reports, stored in
Purview; enable forensic analysis of model decisions.
Monitoring, Audit & Incident
Response
▪ Centralised Logging: Consolidate logs
from Azure Monitor, Application Insights,
and storage audit logs into Azure Sentinel
SIEM.
▪ Continuous Compliance Monitoring:
Azure Policy and Security Center
continuously evaluate resource
compliance against PDPA, GDPR, and
internal standards.
▪ Incident Response Playbooks:
Predefined runbooks in Azure Automation
for containment, investigation, and
remediation of security events; quarterly
tabletop exercises.
Securing MWC’s AI Foundry requires a defense-in-depth approach, embedding security and compliance at every layer: network,
platform, data, and application, to protect sensitive data, meet regulatory obligations, and maintain operational continuity.
4 
6
5

Confidential | www.tha kra lone.co m
Testing & Quality Assurance (1/2)
50
Functional Testing
▪ Infrastructure-as-Code Scanning: Policies enforced in Terraform with Checkov to
Unit & Module Tests: Automated pytest and Jest suites validate individual functions,
model code, and UI components.
▪ Integration Tests: End-to-end tests using Postman and pytest-spark verify data
pipelines, API contracts, and inter-component workflows.
▪ User Acceptance Testing (UAT): Facilitated workshops with MWCI stakeholders to
validate prototype functionality against business requirements; feedback logged in
Azure Boards for resolution.
▪ Test data sets are anonymised or synthetically generated to avoid leakage of sensitive
information.
Performance & Load Testing
▪ Data Pipeline Load Tests: Simulate high-volume ingestions (e.g., 1 million records)
in Fabric to validate throughput and latency SLAs.
▪ API & Model Serving Benchmarks: Use Locust to stress-test inference endpoints,
targeting <200ms p95 latency under concurrency of 100 users.
▪ Front-End Performance: Lighthouse audits and WebPageTest checks for <2s
Time-to-Interactive; bundle size monitoring.
Security & Compliance Testing
▪ Static Application Security Testing
(SAST): Bandit for Python, ESLint security
rules for TypeScript in CI.
▪ Dynamic Application Security Testing
(DAST): OWASP ZAP scans against
staging URLs for common vulnerabilities.
▪ Penetration Testing: Quarterly external
pen tests covering network, API, and UI
layers; remediation tracked in Azure
Boards.
▪ Compliance Validation: Automated
checks via Azure Policy and manual
reviews to ensure adherence to PDPA,
GDPR, and internal security standards.
Rigorous testing and quality assurance are essential to ensure MWCI’s AI Foundry delivers reliable, secure, and performant
solutions. Our approach covers the full testing spectrum—from unit tests to user acceptance—to mitigate risks and uphold service-
level commitments and ensure compliance with data protection regulations such as DPA.
1 3
2

Confidential | www.tha kra lone.co m
Testing & Quality Assurance (2/2)
51
Human-in-the-Loop (HITL) Validation
▪ Domain Expert Sessions: Structured validation cycles where business users
review AI outputs (e.g., generated summaries, ticket categorizations).
▪ Feedback Capture & Triage: Immediate logging of HITL feedback into the
prototype backlog; prioritization and resolution within subsequent sprints.
▪ Periodically run adversarial QA scenarios (e.g., prompt injection or malformed
inputs) to stress-test model resilience.
Regression & A/B Testing
▪ Azure Active Directory Integration: Enforce single sign-on (SSO) and MFA for all
Automated Regression Suites: Nightly CI runs of critical test cases to detect
unintended breakages.
▪ A/B Testing Framework: Controlled experiments for model variants or UI
changes, measuring user engagement, accuracy, and satisfaction to guide rollout
decisions.
▪ Automated checks compare new inference data to training distribution to detect
drift; flagged models are queued for retraining review.
Reporting & Metrics
▪ Test Coverage Dashboards: Visibility into
code and test coverage percentages,
tracked in Azure DevOps.
▪ Quality Gates: Predefined thresholds
(e.g., >80% coverage, zero critical security
findings) enforced before merges and
releases.
▪ Defect Tracking & Metrics: Lead time,
mean time to resolution (MTTR), and
defect density tracked and reported
weekly.
Securing MWC’s AI Foundry requires a defense-in-depth approach, embedding security and compliance at every layer: network,
platform, data, and application, to protect sensitive data, meet regulatory obligations, and maintain operational continuity.
4 
6
5

Commercials

Confidential | www.tha kra lone.co m
Commercial Proposal
53
Service Components 
Indicative Fees
(PHP, VAT Inc.)
• Design, Implement, and Operationalize an
Artificial Intelligence Foundry 
*PHP 34,000,000 to PHP 68,000,000
Terms and Conditions
1. All prices quoted are in Philippine Peso (PHP) and are inclusive of Value Added Tax (VAT)
2. The Scope of Work and commercials indicated in this document are for budgetary purpose only and are estimated based on our current understanding
of the scope. The final Scope of Work, pricing and timelines shall be finalized by Thakral One and MWC upon conducting detailed requirements and
scoping workshop.
3. All payments due to Thakral One shall be made within thirty (30) days from the date of invoice.
4. Thakral One shall require six (6) to eight (8) weeks mobilization after contract signing to onboard resources.
5. Price quoted is valid strictly for continuous execution of the project scope. Any significant delays either in initiation of any individual project phases or
delays in project that are not attributable to Thakral One shall be compensated in addition to the services fees.
6. MWC shall arrange for key project stakeholders and support teams and make available infrastructure and or systems as necessary to facilitate the work
during normal and after business hours and over weekends and public holidays as might be required to deliver the Services.
7. Any drastic change in economic conditions brought about by but not limited to abnormal inflation increase, change in regulatory requirements, FX Rate
devaluation of more than 5%, etc. shall merit a mutual discussion among the parties to agree on an amicable, equitable and reasonable relief based on
the circumstances
8. All other terms and conditions shall be mutually agreed between the parties in the Services Agreement upon awarding of project.
• Duration and price shall depend on the final scope to be agreed between Thakral One and Manila Water.

Confidential | www.tha kra lone.co m
Assumptions (1/2)
54
Scope & Delivery
▪ Consulting across all 8 steps (strategy → CoE blueprint)
is in scope.
▪ Build scope: Phase 1–2 (Foundations + HR/ITSM/KM
MVPs).
▪ Phases 3–4 optional (by change order after P2 gate).
Primary Stack
▪ Microsoft Fabric/OneLake for data & analytics; Azure AI
(Azure ML, Azure OpenAI, AKS/Search) for production
model serving and enterprise LLMs.
▪ Data remains in MWCI’s Azure tenant/regions with AAD
SSO/MFA, Key Vault, Purview, and private networking.
Client Responsibilities (MWCI)
▪ Nominate Executive Sponsor, Product Owner(s),
Domain SMEs; ensure timely decisions and UAT sign-off.
▪ Provide access to Azure tenant/subscriptions, AD
groups, ASAPIO, ManageEngine,
LIMS/SCADA/SharePoint, and network connectivity.
▪ Confirm data processing rights and supply required
content repositories; approve GDPR/PDPA policies and
model risk tiers.
Environment & Tooling
▪ Microsoft Fabric/OneLake available and sized for AI workloads
▪ Purview enabled for catalog/lineage.
▪ GitHub/Azure DevOps, Key Vault, Azure Monitor/App Insights,
Azure API Management available for CI/CD, secrets, observability,
and API governance.
▪ Deployments remain within MWCI-approved Azure regions; no data
egress beyond approved services.
Data Quality & Governance
▪ Source data lands in Bronze
▪ Thakral One transforms to Silver/Gold. Baseline quality & stable
schemas assumed.
▪ PII stays in MWCI tenant; masking & minimization applied; user
consent managed per policy.
Security & Compliance
▪ RA 10173 (Data Privacy Act) compliance; SSO/MFA via Azure AD;
RBAC/ABAC enforced; secrets only in Key Vault.
▪ Pen-tests and security reviews scheduled within phase gates;
remediation follows sprint cadence.
▪ HITL approvals required for high-risk models prior to promotion.
1
2
3
4
5
6

Confidential | www.tha kra lone.co m
Assumptions (1/2)
55
Performance & SLOs (initial)
▪ p95 latency <200ms for core APIs; availability ≥99.9%.
▪ Capacity assumptions: ≤200 concurrent chats, ≤50 QPS
semantic search (re-size if exceeded).
Testing & UAT
▪ Non-prod/prod environments available;
synthetic/anonymised test data provided as needed.
▪ Sev-1/2 defects are go-live blockers unless risk-accepted
by MWCI.
Training & Support
▪ Role-based training (end-user, admin, support) scheduled with MWCI
▪ Hypercare 2–4 weeks post go-live
▪ L1/L2 by MWCI
▪ L3 by Thakral One during engagement.
Exclusions
▪ Legacy remediation beyond integration
▪ Non-Azure hosting; enterprise license procurement
▪ 24×7 managed services after hypercare (unless contracted).
7
9
10
12
Working Model
▪ Scrum, 2-week sprints; Manila core hours 09:00–17:00
PHT.
▪ Blended on-site/remote; secure connectivity provided by
MWCI.
Commercials
▪ Rates exclude taxes/T&E
▪ Invoicing on milestone acceptance
▪ Change control for scope changes, delays, or extra compliance.
8 11

Confidential | www.tha kra lone.co m
High-Level RACI
56
R = Responsible, A = Accountable, C = Consulted, I = Informed
Activity 
ES
(MWCI)
PO
(MWCI)
SME
(MWCI) 
SC DE DL DS UX FE QA SE CL COE
1. Strategy & Roadmap (Step 1) A C C R I I I I I I I C I
2. Maturity Assessment (Step 2) A C C R I I I I I I I C I
3. Data & Tech Foundations (Step 3) A C C C R A I I I C C I I
4. Governance & Compliance (Step 4) A C C C I I I I I I R C I
5. Prototype Use Cases (Step 5 / Phase 2) A R C I C I R R R C I I I
6. Embedding Plan (Step 6) A C C C C R C C C C I R I
7. Human Factors (Step 7) A C C I I I I I I I I R I
8. Continuous Innovation Blueprint (Step 8) A I I I I I I I I I I I R
Phase 1 Build (env, security, CI/CD) A R C I R A I I I C R I I
Phase 2 Build (HR/ITSM/KM MVPs) A R C I C A R R R R C C I
UAT & Go-Live (P1–P2) A R C I C C C C C R C C I
Hypercare & Handover A R I I C R C I I R C C I

About Thakral One

Confidential | www.tha kra lone.co m
About Us
58
Driven by
Collaboration,
Innovation, and
Practical Outcomes.
Thakral One is a technology
consulting and services company
headquartered in Singapore, with a
pan-Asian presence, we focus primarily
around technology-driven consulting,
adoption of value-added bespoke
solutions, enabling enhanced decision
support through data analytics, and
embracing possibilities in the cloud.
years in the industry
16
countries in Asia Pacific & Middle
East with full operations
Headquartered in
Singapore
Strong Presence in Asia
and Emerging Markets
Thakral Group & Thakral One HQ in Singapore.
Thakral One is parented by Thakral Group, one of the most
distinguished and respected families in the Singapore business
fraternity and the global Sikh diaspora.
Serves Banking / FSI, Telco,
Healthcare, Government, &
Consumer-oriented organizations

Data &
Analytics
Services &
Delivery
Technology
Consulting
Solve
Synergise
Support
Clients
How We Help Clients: Our 3S Approach
We adopt a holistic approach to solving complex challenges
by delivering technology-driven consulting. Our team of
industry experts, combined with innovative methodologies
and thought leadership, allows us to provide end-to-end
solutions that drive transformation.
We work in synergy with our clients, helping them adopt
cutting-edge data and digital capabilities. Through the
deployment of data driven solutions and the establishment
of comprehensive support systems, we ensure seamless
execution and long-term success, guiding our clients every
step of the way.
By harnessing the power of analytics, we enable value-
driven decision-making, empowering businesses to achieve
sustainable, measurable outcomes that align with their
strategic goals
Confidential | www.tha kra lone.co m 
59

Confidential | www.tha kra lone.co m
Our Services & Offerings
60
Technology Strategy & Roadmap
Helping clients develop actionable technology strategies,
long-term IT roadmaps, and digital transformation
journeys.
Architecture & Design
Guiding clients in designing scalable, secure, and efficient
technology architectures.
Technology Selection & Implementation
Consulting on the selection and implementation of
software solutions, cloud platforms, and enterprise
applications.
Governance & Project Management
Ensuring successful delivery with governance
frameworks, risk management, and operational efficiency.
Automation & Innovation
Implementing automation solutions and leveraging AI/ML,
analytics, and other emerging technologies.
Data Strategy & Governance
Helping clients define clear data strategie s and establish strong data governa nce
frameworks that e nsu re data a ccuracy, security, and co mp liance.
Data Architecture & Infrastructure
Designing and implementing sca lable and flexib le d ata ar chitectures ( clou d, hybrid,
and on-pre mise solutions) that align with business o bjectives.
Advanced Analytics & AI/ML
Levera ging predictive an alytics, mach ine learning , and AI to help clients ga in
actionable insights an d fore cast future trends.
Business Intelligence (BI) & Data Visualization
Ena bling clie nts to turn data into business insights throug h B I platforms (e.g., Po wer
BI, Tablea u) and cu stom dashboard s.
Big Data Solutions
Designing big data platfo rms and leverag ing techno logies like Hadoop, Spark, a nd
cloud data lakes to ma nage and ana lyze massive datasets.
Data Integration, Migration, & Automation
Integrating disparate data so urces, data mig ration, au tomating data pipelin es, and
ensurin g r eal-time d ata availab ility for be tter decision-making.
Efficiency & Value Advisory
Guiding clients in designing IT architecture, optimizing
resources, and selecting systems to enhance cost-
effectiveness, performance, and value realization.
Optimization for Efficiency
Our Managed Services, combined with Technology
Consulting, empower organizations to prioritize core
business initiatives while ensuring IT infrastructure is
optimized for efficiency
Managed Services
Providing dedicated teams to seamlessly manage IT
operations, enabling focus on core initiatives and
growth, with scalable services that adapt to evolving
needs.
People & Talent
Integrating top-tier talent into client teams to ensure
smooth collaboration and efficient project execution,
ensuring timely delivery and budget adherence.
Technology Consulting 
Data & Analytics 
Services & Delivery

Confidential | www.tha kra lone.co m
Awards & Testimonials
61
Singapore Business Review
Technology Excellence Award 2024
Recognised for implementing an automated credit
decisioning system at ABBANK Vietnam, significantly
enhancing the bank’s credit decisioning process.
“The cooperation with experienced partners such
as Thakral One, will help ABBANK not only to
efficiently implement and operate SAS solutions,
but also learn more practical implementation
approaches at home and abroad.”
Singapore General Hospital
Thakral One developed a disease
surveillance dashboard for Singapore
General Hospital to track staff-related
respiratory infections, aiding in early detection
and containment of COVID-19
“The work done by Thakral One is very
much appreciated! At a time when we were
struggling to develop and maintain the
rapidly escalating COVID-19 data systems,
they stepped in to support us.”
Mizuho Bank
Thakral One managed the release
and configuration of system
environments for a Mizuho’s core
banking system implementation,
ensuring timely and budget-friendly
delivery.
“The support from your people has
been a key success factor of T24
implementation of Mizuho Bank
Malaysia Subsidiary.”
Mr. Nguyen Manh Quan,
Acting General Director of ABBANK
Dr. Indumathi Venkatachalam,
Department of Infectious Disease
Hiroshi Motoyama,
President & CEO of Mizuho Bank
Globe Telecom
Thakral One was among the
Globe 100 (Top Business
Partners) awardees for 2021 and
2023, out of its 1,500+ Partners.
Awarded as an Analytics Catalyst
Partner in 2018, Thakral One
has continuously supported
Globe's business potential and
delivered top-tier experience to
its subscribers.

Appendix 1: 8 Step Framework
Detailed Approach and Methodology

Step 1: Enterprise AI Adoption Strategy

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: Enterprise AI Adoption Strategy
64
Deliverables
▪ Enterprise AI Strategy document – outlining vision, strategic
objectives for AI, and alignment to business strategy. CXO sign-off
on AI strategy with approved funding plan.
▪ AI use case portfolio – a prioritized list of promising AI initiatives
with expected benefits (6–10 use cases, quick wins <3 months
ROI and strategic bets >12 months) identified and costed. d a
phased execution plan.
▪ Roadmap & investment plan – timeline of AI projects (pilot to
scale), required investments, and key milestones for 12–24+
months.
▪ AI governance charter (initial) – high-level framework for
oversight, ethics, and risk management to be detailed in Stage 4.
▪ Executive buy-in artefacts – e.g. strategy presentation for
board/CXO, and a communication plan to cascade the AI vision
enterprise-wide. Enterprise AI as fixed board agenda update, with
nominated sponsor and reporting cadence agreed.
Key Activities & Tasks
▪ Define an AI vision tied to business objectives (growth, efficiency, risk
management) and articulate how AI will create value in core areas.
▪ Identify priority use cases via executive workshops to where AI can solve
real business problems (e.g. predictive maintenance) and produce quick
wins.
▪ Assess competitive & industry landscape for AI (including peers’
initiatives and emerging trends) to inform a differentiated strategy.
▪ Develop a high-level AI adoption roadmap – sequencing initiatives in
short, mid, long term. This includes “no-regret” foundational investments
and pilot projects for early value. Identify AI evangelists and boot up an AI
Think Tank to closely track strategic change and mood.
▪ Establish governance at the outset: form an AI steering committee or
council to sponsor the program, set guiding principles, and align AI efforts
with enterprise risk appetite and compliance needs.
▪ Ensure CEO and leadership sponsorship – secure commitment for
necessary funding, talent, and organizational changes. Leadership must
champion AI use and set the cultural tone for adoption.

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: Enterprise AI Adoption Strategy
65
Resources Involved (Vendor & Client)
▪ Vendor: AI strategy consultants, industry domain experts, data
strategy leads – to bring external best practices and facilitate
workshops.
▪ Client: C-suite executives (sponsor, CEO, CIO, CDO), business
unit heads, and strategy or innovation team – to provide business
context, make decisions, and champion the AI vision. Support
from finance (for ROI/business case input) and compliance/risk (to
anticipate regulatory considerations) is also key.
Key Outcomes
▪ A clear enterprise AI game plan tied to business value – ensuring
everyone from executives to project teams understand the AI “north
star” and priorities.
▪ Executive alignment and sponsorship for AI initiatives, with
funding and resources earmarked. The organization has a compelling
AI narrative (“why now, to achieve what”) that reduces hype and
focuses on practical outcomes.
▪ A prioritized portfolio of AI use cases to pursue, balanced between
quick wins and strategic, transformative bets.
▪ By the end of this step, AI is positioned as a board-level strategic
initiative with a roadmap, rather than ad-hoc experiments, setting the
stage for structured execution.
Timeline & Effort
▪ Approximately 4–6 weeks of focused effort to develop the
strategy and roadmap. This includes interviews with stakeholders,
strategy workshops, and iterative reviews with leadership.
▪ The effort is front-loaded with senior stakeholder time (e.g.
several CXO workshops) and a dedicated consulting team. Early
alignment may run in parallel with initial data assessments to
accelerate subsequent stages.

Step 2: AI Maturity Assessment & Ladder

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: AI Maturity Assessment & Ladder
67
Deliverables
▪ AI Maturity Assessment Report: A comprehensive document detailing the
current maturity level for each dimension with a heat-map and radar chart
visualization. It will include qualitative findings and quantitative scores.
▪ Maturity “Ladder” Diagram: A simplified visual that shows the current rung
on the AI maturity ladder and the target rung.
▪ Gap Analysis Matrix: A structured list of identified gaps along with
proposed actions to close each gap. For example: “No formal AI governance
-> establish AI Governance Board in next 3 months” or “Limited AI skills ->
hire X data scientists + training program (Stage 7)”.
▪ Recommendations & Roadmap Addendum: Concrete recommendations
feeding into the overall AI roadmap. We will also suggest quick fixes for
“low-hanging fruit” gaps.
▪ Executive briefing on readiness: A high-level summary for sponsors,
highlighting readiness to proceed and any prerequisites for success. For
example, if the assessment finds that data readiness is very low, leadership
should be informed that significant work in Stage 2 is non-negotiable before
expecting pilot successes.
Key Activities & Tasks
▪ Develop a maturity framework: Working with legal & compliance experts from the
relevant industry sector, develop an industry aligned AI maturity model to define custom
criteria relevant to the organization’s context (specific regulatory compliance
capabilities).
▪ Data gathering: Conduct structured interviews and workshops with key stakeholders
across IT, data science teams, business units, and compliance. Collect information on
current AI use cases (if any), data practices, skill levels, leadership support, and past
pilot results. Also use questionnaires or checklists to quantitatively score each
dimension.
▪ Assessment of dimensions: Evaluate the organization against each dimension in the
model:
▪ Strategy: Is there a clear AI vision and executive sponsorship?
▪ Data: Are data assets unified, of high quality, and accessible for AI?
▪ Technology: Is infrastructure scalable with tools for deploying AI in production?
▪ Organization: Are roles and structures (like an AI center of excellence) in place?
▪ Talent: Does the workforce have (or can we hire) the necessary data science
and ML skills?
▪ Benchmarking: Compare findings with industry benchmarks and best-in-class
examples. Understanding how peers or AI leaders perform on the maturity scale to
determine competitive gaps.
▪ Maturity level determination: Position the organization on the maturity ladder. against
evidence gathered to justify placement.
▪ Gap analysis and recommendations: Determine the maturity goal and determine
what’s needed to move to the next maturity level. Identify key gaps (skills, processes,
tech) to address. Prioritize these gaps and link them to later stages of the framework
(e.g. if governance is lacking, Stage 4 will be critical).

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: AI Maturity Assessment & Ladder
68
Resources Involved (Vendor & Client)
▪ Vendor: Experienced consultants or assessors who know the chosen maturity
model, with sector expertise (to gauge regulatory-specific criteria). They facilitate
interviews, conduct analysis, and bring outside perspective on what “good” looks
like at each maturity level.
▪ Client: Stakeholders across the organization: IT leaders, head of data/analytics,
compliance officer, business function heads, and team leads of any existing AI or
analytics teams. Their honest input is crucial. The client should also designate a
coordinator (e.g. from the strategy or analytics office) to help gather internal
documentation and schedule interviews.
▪ Additionally, domain regulators or auditors: In highly regulated sectors,
consulting with internal audit or risk management teams can validate whether the
maturity in governance or compliance meets industry norms.
Key Outcomes
▪ Baseline clarity: Executive-level visibility on AI strengths, bottlenecks, and
blockers to scale. The organization gains a clear understanding of its AI
readiness. This avoids blindly moving forward and later discovering critical
weaknesses. E.g. if the assessment reveals data is not as “AI-ready” as assumed,
the team can recalibrate priorities (perhaps invest more in data readiness before
chasing too many pilots).
▪ Targeted improvement plan: Actionable investment plan aligned to value
acceleration and risk mitigation Rather than generic advice, the organization now
has a tailored set of actions to improve capabilities. This improves efficiency,
investments and efforts can be channelled to the areas that will most raise
maturity (be it talent, technology, or processes).
▪ Alignment on expectations: C-suite consensus on realistic AI scale-up path
and ownership. The maturity assessment helps set realistic expectations with
executives. If the organization is at an early stage (e.g. “Ad hoc” experimenters,
which describes a majority of companies), leadership will understand that large-
scale ROI will take time and foundational work. Conversely, if the firm is already
advanced in some dimensions, they can leverage those strengths.
▪ Benchmark against best practices: By referencing proven frameworks
developed by industry experts and aligned with industry benchmarks, the
company measures itself against a credible standard.
▪ Overall, this stage ensures that subsequent efforts in the AI adoption journey are
grounded in the organization’s reality, with a ladder to climb and a way to
measure progress over time.
Timeline & Effort
▪ Roughly 4-6 weeks for a thorough maturity assessment, which can overlap
slightly with foundation building. The timeline depends on organization size and
availability of stakeholders.
▪ The effort involves information gathering in the first 2–3 weeks (interviews,
surveys) followed by analysis and report generation in the remaining time.
Management reviews and iteration on the findings are important so the
assessment is seen as accurate and fair by the organization to secure buy-in for
addressing gaps.

Step 3: AI Technology & Data Foundations

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: AI Technology & Data Foundations
70
Key Activities & Tasks
▪ Data strategy & architecture: Design a unified data architecture that breaks down
silos into a data lakes or warehouses (lakehouse) as a single source of truth for AI
systems) into a common platform, with mechanisms for real-time or batch data
ingestion.
▪ Data governance and quality: Implement data governance policies (metadata
management, data catalogue, lineage tracking) & data quality processes (cleaning,
normalization) to ensure AI models train on reliable, unbiased data. Embed privacy
controls (encryption, access controls, anonymization) at the design phase.
▪ Technology stack selection: Choose appropriate AI/ML platforms and tools (cloud AI
services, on-premise GPU infrastructure, ML frameworks) aligned with enterprise IT
standards & scalability needs. Leverage technologies that support MLOps, model
serving & versioning. The stack will support compliance requirements via audit logs for
data/model usage.
▪ Infrastructure setup: Establish environments for development, testing & production.
Involves setting up cloud environments or hybrid infrastructure with necessary compute
power. Emphasise resilience, security (zero-trust, data protection), & the ability to
handle large datasets typical in AI training.
▪ Data integration & APIs: Develop integration layers (APIs, ETL pipelines) to connect
AI solutions with legacy systems. Integrate middleware or connectors so that AI insights
can flow into existing workflows. Plan for integrating AI outputs into core applications.
▪ Baseline tools & frameworks: Install & configure foundational tools such as data
science workbenches, model development environments, and collaboration platforms.
Provide sandbox environments for data scientists that are compliant with IT controls
Deliverables
▪ Data architecture blueprint: Documentation of target data architecture
(data lake/warehouse design, data flows, sources) and integration
approach.
▪ Technology architecture & stack document: A detailed view of the
chosen AI platform, infrastructure, tools, and how they interoperate
(covering dev, test, prod). This includes cloud architecture diagrams or
on-premise infrastructure plans.
▪ Data inventory & governance policies: An initial catalogue of
available data for AI and governance guidelines (data ownership,
access rights, data retention and anonymization policies, etc.).
▪ Proof-of-foundation setups: Initial instances of infrastructure
components – e.g. a pilot data lake with sample data, a configured ML
development environment validating foundations meet requirements
(throughput, security checks).
▪ Compliance & security checklist: A mapping of how the data/tech
foundation adheres to relevant regulations including any required
certifications or assessments.

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: AI Technology & Data Foundations
71
Resources Involved (Vendor & Client)
▪ Vendor: Data architects, cloud architects & AI platform engineers from
consulting & technology practices, to design and set up technical
components.
▪ Client: IT infrastructure teams, enterprise architects, & data engineering
teams, to provide knowledge of existing systems & implement
integration. Data owners from business units may be involved to ensure
critical datasets are identified and prepared. Security and compliance
officers play a role in approving designs (ensuring they meet regulatory
standards for data handling).
Key Outcomes
▪ A scalable, secure data and AI platform ready to support development
and deployment of AI use cases. The enterprise now has accessible,
consolidated data (breaking down silos) and the computing environment
needed for AI experiments & models. A hybrid / multicloud platform for
flexibility and sustainability (green AI) as AI is very heavy on compute
power and MNCs will have ESG targets to manage.
▪ Improved data accessibility and quality: Data that was once
fragmented is now integrated into a single source of truth, governed by
clear policies. This not only benefits AI projects but also enterprise
analytics in general. Data Lineage & Auditability, especially for regulated
industries, is critical to track how data is transformed, accessed, and
used in training/inference.
▪ Reduced technical risk: By proactively addressing infrastructure,
integration, compliance, identity & access management (IAM) the
organization lowers the risk of AI projects failing due to technical
limitations. The foundation ensures that as pilots succeed, they can be
scaled without a complete redesign.
▪ Readiness for AI experimentation: Teams of data scientists and
engineers have the tools and environments to start prototyping AI
solutions quickly (sandboxes with needed data and compute). This
accelerates subsequent stages and instils confidence that the
organization’s technology can handle AI at scale.
Timeline & Effort
▪ Typically 6–10 weeks for initial foundation build-out, though it may extend
if legacy integration is complex. Early tasks (data discovery, architecture
design) overlap with strategy phase outputs.
▪ Stand up core infrastructure in 4–6 weeks, followed by iterative
enhancements. This stage often runs in parallel with use case selection
(Stage 5), so foundational work can support upcoming prototypes. The
effort requires significant hands-on from technical teams (both vendor
and client IT), with periodic review by security/compliance stakeholders.

Step 4: AI Governance Framework

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: AI Governance Framework
73
Deliverables
▪ AI Governance Charter: A formal document approved by leadership
outlining the governance framework – its purpose, scope, and the
composition of the governance committee. It will detail roles (who is
responsible for what) and governance processes.
▪ AI Policy Manual / Guidelines: A set of approved policies covering ethics,
data, model development, testing, deployment, monitoring, &
decommissioning. These must be written in consultation with legal &
compliance departments.
▪ Regulatory Compliance Mapping: Documentation that shows how the
AI governance addresses relevant laws (e.g. an appendix that maps
each article of the regulations to company controls), useful for internal
audit or external regulators to demonstrate diligence. Mapped and closed
gaps vs. GDPR, DPA (PH), EU AI Act; audit-ready compliance posture for
all high-risk AI.
▪ AI project review workflow: A defined workflow (flowchart or RACI matrix)
for AI project initiation and approval.
▪ Templates and tools: Deliver standard templates such as a Model
Documentation Template (covering intended use, data, performance, bias
checks, etc.), and a Risk Assessment Checklist for AI solutions. If any
software tools are selected (e.g. for monitoring), deliver initial configurations
or pilot usage of those tools.
Key Activities & Tasks
Establish Governance Structure: Form an AI Governance Committee with stakeholders
from IT, data science, legal, compliance, and business units. Assign specific roles (e.g. AI
Ethics Officer, Governance Lead), and define reporting lines (e.g. to Enterprise Risk or
Board-level Risk Committee).
Develop AI Policies and Guidelines: Draft and approve governance policies covering:
▪ Ethical AI principles – fairness, transparency, accountability
▪ Data usage & privacy – consent, anonymization, legal compliance (e.g. GDPR, HIPAA)
▪ Model risk management – validation protocols, documentation (e.g. model cards),
human oversight
▪ Bias & fairness testing – regular assessments and remediation procedures
▪ Security – protection from breaches, adversarial threats, and IP leakage
Map Regulations to Practices: Translate regulatory requirements into operational
controls. Create a compliance matrix mapping industry-specific obligations (e.g. EU AI Act,
SR 11-7) to governance mechanisms.
Implement Oversight Processes: Define formal governance procedures, including:
▪ AI use-case review workflows (e.g. approval gates for high-risk deployments)
▪ Scheduled audits of AI systems for compliance, fairness, and performance
▪ Central AI registry tracking all production models, intended use, and validation status
Training & Awareness: Conduct role-specific training for developers, data scientists, and
business teams on governance requirements. Build awareness of escalation paths for
ethical or compliance issues.
Tooling Support: Integrate tools that enable practical compliance, such as:
▪ Bias detection during development
▪ Model documentation templates
▪ Differential privacy solutions
▪ Monitoring tools for model drift, accuracy, and risk indicators

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: AI Governance Framework
74
Resources Involved (Vendor & Client)
▪ Vendor: Consultants specialised in AI risk and governance (possibly 3
rd 
party experts)
to bring knowledge of best practices and evolving regulations, helping to craft policies
that meet both compliance needs and business practicality. Legal advisors may be
engaged to interpret new laws and ensure policies are robust.
▪ Client: Chief Risk Officer or Chief Compliance Officer and teams, Data Privacy Officer,
Legal counsel, and representation from IT security. Heads of AI/analytics teams and
business unit leaders to ensure the framework is practical and addresses real use
cases involving those who will implement it, to balance controls with innovation.
▪ A cross-functional working group will be formed (facilitated by vendor experts) to
draft the framework and attain senior management approval. The client should also
plan for resourcing the ongoing governance.
Key Outcomes
▪ Risk mitigation and trust: The enterprise will have a formal mechanism to
ensure AI solutions are safe, ethical, and compliant, thereby avoiding costly
missteps (e.g. a biased AI model causing reputational damage or legal issues).
This is especially important in regulated sectors where a single AI error can
have serious consequences.
▪ Regulatory readiness: With the governance framework, the organization is
prepared to meet current and upcoming regulations. If audited or if new laws
come into effect, the ability to demonstrate control processes are in place. This
proactive stance can even be a competitive differentiator in gaining customer
trust or faster regulatory approvals for AI-driven products.
▪ Structured oversight and accountability: Clear accountability for AI projects
with a defined committee & officers who oversee AI initiatives. This oversight
encourages discipline: teams know from the start that certain criteria must be
met, which improves the rigour of model development (e.g. proper validation,
documentation). It also fosters cross-department communication (e.g. IT,
compliance, business all collaborate via the governance board).
▪ Balanced innovation with control: A well-crafted governance framework
does not stifle innovation; instead, it enables sustainable innovation. By
setting boundaries and guardrails, it gives executives confidence to scale AI
(knowing risks are managed) and gives teams guidance on how to innovate
responsibly.
▪ This step builds the “rules of the road” so that subsequent AI deployments
(Stages 5–8) happen in a controlled, repeatable manner. The organization can
move faster with AI because it has the brakes and steering in place.
Timeline & Effort
4–8 weeks to design & roll out the initial governance framework. The timeline can vary
depending on how many stakeholders need to review/approve (in regulated contexts,
multiple iterations with Legal and Risk committees are common).
▪ Drafting policies (by a small core team of 2–4 people), and broader consultations or
workshops to vet the proposals with stakeholders. If regulatory compliance is complex,
expect more time for legal review. Rolling out training/awareness might extend beyond
this timeframe but should begin in this stage.
▪ Governance development can run in parallel with pilot use case development (Stage 5),
but must be in place before large-scale deployments (Stage 6) to avoid mistakes.
Many firms choose to implement at least minimal governance checkpoints even for
prototypes, to build the discipline early.

Step 5: AI Prototype Drive Use Cases

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: AI Prototype Drive Use Cases
76
Deliverables
▪ Use Case Catalogue: A curated list of AI use cases evaluated & selected
for prototyping, including a brief for each (objectives, value case, feasibility
notes). This can be an update to the portfolio from Step 1, now with more
detail and priority ranking.
▪ Prototype solutions: For each chosen use case, a working prototype or
demo is rapidly developed in a sandbox or test environment. The
deliverables illustrate the core functionality and prove working hypotheses.
▪ Pilot results report: Documentation of each prototype’s outcome, covering
approach, results with quantitative metrics and qualitative feedback from
users, with recommendation e.g. “Proceed to pilot deployment” or “Re-
evaluate after addressing data issues”..
▪ Business case update: For prototypes that will move to implementation,
update the business case assumptions with real findings to refine the ROI
projection. This updated business case is useful to secure funding for
scaling in Stage 6.
▪ Technical artefacts & code: The code, datasets, and any developed IP
from prototypes should be captured and stored in a repository. Even for
prototypes that aren’t pursued, these artefacts are knowledge for the
organization (showing what was tried and why it did/didn’t work, avoiding
repeated mistakes).
Key Activities & Tasks
▪ Use Case Identification & Prioritization: Source potential AI use cases from Stage
1’s portfolio and business unit inputs. Evaluate based on value (e.g. cost savings,
revenue uplift, risk mitigation), feasibility (data quality, technical complexity), and
strategic alignment. Prioritize cases with measurable outcomes and clear business
relevance.
▪ Use Case Deep-Dive & Selection: For shortlisted use cases, define the problem,
expected benefits, success KPIs, data sources, and key stakeholders. Ensure each
has a business sponsor. Select a mix of “quick wins” and high-impact strategic cases
to build momentum and experience.
▪ Prototype / PoC Development: Form agile teams to build rapid prototypes using
existing data foundations (from Stage 2). Scope narrowly focus on demonstrating core
logic or feasibility. Use short sprints and involve domain experts early. Examples: early
ML model, basic workflow automation, or chatbot MVP.
▪ Business Involvement & Feedback: Involving end-users throughout development to
test relevance and usability. Elicit feedback from business teams to improve prototype
fit, surfaces design refinements, and builds early champions for adoption.
▪ Evaluation of Prototypes: After 4–8 weeks, assess each prototype on defined
metrics (e.g. model accuracy, process time saved), integration fit, and user feedback.
Identify blockers (e.g. data issues, workflow misalignment) and determine feasibility
for scale.
▪ Iterate or Kill Decisions: Decide whether to advance, refine, or halt each prototype.
Document lessons learned (e.g. gaps in data pipeline or engagement). For scalable
cases, note what needs to improve before implementation—this feeds directly into
Stage 6 design.

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: AI Prototype Drive Use Cases
77
Resources Involved (Vendor & Client)
▪ Vendor: Data scientists, AI engineers & solution architects from the consulting
side to lead model development & rapid prototyping. Possibly UX designers or
business analysts to help shape prototypes and gather user feedback.
▪ Client: Business domain experts (who understand the process/problem) working
alongside the vendor team ensuring the solution is grounded in reality and help
interpret model outputs. Also, client data engineers to facilitate data access and
any IT personnel needed to set up environments or integrations. Each prototype
must have a business owner or champion from the client side driving it (ensuring
it addresses the right problem and helping to remove internal roadblocks).
▪ This stage often adopts a hub-and-spoke model: a central AI team (the hub,
initially vendor-led) working with individual business unit teams (the spokes) on
their specific prototypes. This builds internal capabilities as client team members
learn by doing with vendor experts.
Key Outcomes
▪ Validated use cases with evidence: The organization moves from ideas to
concrete evidence. Successful prototypes validate that an AI solution can work
and is worth investing in. Even failures are valuable – they prevent wasted
resources later and often highlight what needs to change (e.g. data improvements
or process changes). Introduce a “light governance gate” even for prototypes —
bias checklists, basic data lineage, stakeholder consent, and explainability
thresholds.
▪ Stakeholder buy-in through tangible results: Seeing functioning prototypes
and initial results helps convert sceptics into supporters. Department heads and
end-users are more likely to champion an AI project that they have seen working
on their data, rather than just hearing theoretical promises. Building needed
support for broader adoption.
▪ Refinement of requirements: Prototyping surfaces practical considerations for
implementation. These insights inform the design of the full solution in Stage 6,
making it more likely to succeed. It also tests the earlier groundwork: e.g., did the
data pipeline from Stage 2 perform well? Any issues found can be addressed
now.
▪ Accelerated learning & skill development: Through hands-on, joint
development, the internal team’s competency grows. Data scientists and
engineers learn new tools and techniques, business users get comfortable
working with AI outputs, and the organization’s overall “AI fluency” increases. This
human capital aspect is important for long-term AI adoption.
▪ Step 5 is where plans turn into pilots. By its end, the company has a few prototype
AI solutions and a clearer view of their business value. This lowers risk for scaling
and shapes an invested Roadmap, as only approved use cases proceed beyond
the prototype stage.
Timeline & Effort
▪ Expect 4–8 weeks per prototype for initial development and evaluation. Multiple
prototypes can run in parallel if resources allow (e.g. two teams tackling two use
cases concurrently). Typically, a batch of 2–3 prototypes might be executed in a
10–12 week window.
▪ This is an intensive, hands-on period utilising Agile methodologies (weekly sprints,
quick daily stand-ups) to maintain speed.
▪ At the end of this stage, teams present their prototypes and results to executives
– to demonstrate progress and glean support for scaling the successful ones.

Step 6: Embedding Meaningful AI

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: Embedding Meaningful AI
79
Deliverables
▪ Production AI System: The fully deployed AI solution accessible to users -
live application, an API service, a dashboard with AI insights, or a model
integrated into an existing software. This is the centerpiece deliverable: the AI
“in action” in the business environment.
▪ Deployment & integration documentation: Technical docs describing final
system architecture, integrations, APIs, data flows, and how to operate the
system. Also includes any runbooks for operations (e.g. restart procedures,
failure recovery, backup processes).
▪ User manuals/training materials: Documentation and job-aids for end-users
of the AI system - quick reference guides, how-to videos, or an internal
knowledge base article explaining what the AI does and how to interact with it.
▪ Support & monitoring plan: A documented plan for ongoing support,
including designated support contacts, schedule for model retraining or review
(if applicable), and monitoring metrics with responsible persons. If an AI Ops
dashboard was set up, screenshots or links to it, along with explanation of what
is monitored (e.g. data drift, response times, etc.).
▪ Go-live report: A summary report at the point of full deployment outlining the
process taken, results of final testing, and the initial impact observations.
Evidence that the solution met its acceptance criteria and went live with
stakeholder consent.
▪ Case study for internal use: A brief narrative of the journey of this use case
from prototype to production, highlighting the value achieved. including
problem, solution, and results in business terms.
Key Activities & Tasks
▪ Solution hardening & scale-up: Enhance prototypes for production—optimise
models, write robust code, test edge cases, and ensure scalability to handle real-world
data volumes and performance demands.
▪ Technology integration: Embed AI into core IT systems via APIs, middleware, or
RPA. Automate data flows and ensure seamless triggering of downstream actions
(e.g. CRM updates, dashboards). Collaborate closely with IT teams.
▪ Production deployment: Move AI systems into secure, scalable environments
(cloud/on-prem), using MLOps pipelines, containerization & monitoring. Establish the
infrastructure for ongoing operations and iteration.
▪ Testing & validation: Run UAT, load tests, and real-world validation against business
KPIs. Confirm model performance and integration readiness. Include governance
reviews (e.g. bias, ethics) before go-live.
▪ User training & change management: Deliver structured training, embed AI
champions, and clarify new workflows. Using pilot success stories to drive adoption.
Run in parallel with legacy processes if needed to build trust.
▪ Operational support setup: Define ownership, support teams, and escalation paths.
Set up dashboards and alerts to monitor performance, drift, and usage. Assign
accountability for ongoing AI operations.
▪ Impact measurement: Track and report post-deployment metrics (e.g. accuracy,
efficiency gains). Compare against baseline to quantify business value. Share early
wins to reinforce buy-in and support broader AI adoption.

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: Embedding Meaningful AI
80
Resources Involved (Vendor & Client)
▪ Vendor: Software and cloud/DevOps engineers lead deployment and integration
(if client lacks in-house capacity). Data scientists may assist with final model
tuning. UI/UX developers may be needed if interfaces are required. Vendor
project manager oversees the go-live.
▪ Client: IT application owners ensure the AI integrates cleanly into enterprise
systems. Business process owners and end-users test and validate the solution.
Security and compliance teams perform final reviews, especially if new data or
cloud elements are involved. Operations/maintenance teams must be trained or
assigned—often forming an AI Ops function to eventually manage the solution.
▪ Vendor and client typically operate as a single, integrated team during this stage.
A clear knowledge transfer plan is essential, ensuring the client can fully own the
solution post-deployment (even if vendor support continues temporarily).
Key Outcomes
▪ Operational AI delivering value: AI is now embedded in the business, solving
real problems daily. The organization moves beyond prototypes to live systems
in production. This is a major milestone—few enterprises reach it. Those that do
often see significant gains.
▪ Enhanced business process performance: With AI deployed, targeted metrics
show measurable improvement—faster operations, better decisions, cost
savings, or higher customer satisfaction. These results validate the AI strategy
and strengthen the case for future investment (nothing builds trust like real
operational outcomes).
▪ Integration of AI into culture: Users begin to trust and rely on AI in their
workflows. This cultural adoption is critical—it means AI is making jobs easier or
decisions better. AI becomes embedded in how work gets done.
▪ Lessons for scaling next use cases: Deployment provides critical learnings:
reusable playbooks for MLOps, templates for integration, and improved change
management processes. It also highlights any needed org changes (e.g. an AI
Ops team) to support ongoing scale-up.
▪ Crossing the chasm to execution: By the end of Step 6, the enterprise should
have at least one AI system fully deployed and delivering benefits—breaking out
of “pilot purgatory” and creating a foundation for repeatable, scalable AI adoption
across the business.
Timeline & Effort
8–16 weeks to productionize a prototype. Simple use cases (low integration) are
faster; complex ones (multiple systems, heavy data) take longer. Phased rollout
(e.g. by region) may extend timelines.
▪ Concurrent work: Development, integration, training, and change management
usually overlap. Avoid rushing—ensure users are trained and ready, potentially
via a short pilot run.
▪ Stabilization: Allocate 2–4 weeks post-launch for issue resolution and fine-
tuning. Include this buffer in the overall timeline.

Step 7: AI – Human Factors

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: AI – Human Factors
82
Deliverables
▪ Change Management Plan: Strategy document outlining how changes will be
handled for each AI rollout. Covers stakeholder analysis, communication
cadence, training, and resistance mitigation. Frameworks like ADKAR
(Awareness, Desire, Knowledge, Ability, Reinforcement) may guide activities.
▪ Communication materials: FAQs to debunk AI myths, leadership
memos/videos, and slide decks for managers to cascade messaging. Ensures
clear, consistent communication across the organization.
▪ Training curriculum & records: Structured curriculum detailing courses,
workshops, and materials. Tracks delivery and participation Materials (slides,
guides) included for reuse.
▪ HR policy updates: Revised policies & job descriptions where AI impacts
roles. May include responsibilities for validating AI outputs or updated KPIs
aligned with human-AI collaboration.
▪ Feedback & adoption metrics: Metrics on tool usage, user satisfaction, and
sentiment shifts. Delivered via dashboards or reports to monitor adoption
progress and address issues early.
▪ Workforce transition roadmap: Long-term plan to manage workforce shifts
due to AI. Includes upskilling, reskilling, and hiring strategies to align talent with
evolving roles.
Key Activities & Tasks
▪ Change management planning: Define a clear strategy aligned with AI rollout.
Identify affected stakeholder groups and address concerns with tailored, transparent
messaging with clear emphasis that AI supports employees, not replaces them.
Communicate clear changes and benefits.
▪ Executive and leader advocacy: Secure visible sponsorship from senior leaders.
Leaders should promote AI adoption, model usage, and reinforce a vision of
meaningful work improvement. Encourage open dialogue to surface concerns and
build trust.
▪ Employee engagement & involvement: Involve staff early through workshops,
feedback, and team-based AI champions. This builds ownership and reduces
resistance. Recognize contributions to reinforce engagement and momentum.
▪ Up-skilling and training programs: Deliver tiered training—awareness for all, role-
based for users, advanced for technical staff. Cover AI literacy, interpretation, and
application. Use blended methods: e-learning, internal sessions, vendor-led courses.
▪ Job design and role alignment: Reassess how AI alters responsibilities. Update
roles and KPIs with HR, focusing on higher-value work. Create reskilling paths where
needed (e.g. to AI supervision roles), reinforcing augmentation, not displacement.
▪ Continuous feedback and support: Enable two-way feedback via surveys, forums,
and inboxes. Adapt training and tools based on input. Provide ongoing support
through internal help desks or peer groups.
▪ Cultural initiatives: Foster a learning culture with success stories and visible wins.
Encourage experimentation and reward AI advocates. Over time, embed “AI-ready”
behaviors into the organizational mindset.

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: AI – Human Factors
83
Resources Involved (Vendor & Client)
▪ Vendor: Change management consultants, organizational development experts,
or HR transformation specialists (recommend having dedicated change teams
for AI rollout). They design and help execute the change program, bringing
structured methods and outside insight. External trainers or AI e-learning
providers may also support delivery if needed.
▪ Client: Human Resources (especially L&D), Corporate Comms, and line
managers. HR/L&D lead training and skills programs; Comms teams ensure
messaging is clear and consistent. Line managers and team leads are
essential—acting as the bridge to staff and champions of AI. Internal IT/data
leaders partner with HR to tailor training content and ensure relevance.
▪ Employee reps or forums (e.g. unions or councils) may also be engaged to
gather feedback and gain alignment on AI-related changes, especially in
regulated or unionized environments.
Key Outcomes
High user adoption and engagement: Effective change management and
training increase employee uptake of AI tools. Instead of resistance or
workarounds, AI becomes actively used in daily workflows. Adoption metrics should
rise steadily, with more positive feedback. AI shifts from being imposed to being a
welcome support.
Reduced fear & resistance: Transparent communication and involvement help
dispel fears (e.g., “AI will take my job”). Employees understand the purpose of AI
and feel heard. This prevents morale issues and pushback that can arise if AI is
rolled out poorly. Cultural resistance must be addressed for AI to have real impact.
Empowered & skilled workforce: Upskilling builds new competencies—data
analysis, AI interpretation, critical thinking—that improve agility and job satisfaction.
Employees spend less time on repetitive tasks, more on value-added work. Co-
creating and learning to use AI can boost productivity and retention.
AI aligned with organizational culture: Company culture becomes more data-
driven and innovative. Employees begin to trust AI tools, balanced with human
judgement. A mindset of experimentation and learning emerges—staff are open to
new tech, knowing they’ll be supported.
Human-AI synergy achieved: Step 7 ensures AI is embedded into the human
fabric of the organization. The result is a symbiotic environment where AI supports
people, and people enhance AI, driving sustainable value, especially important in
regulated sectors where trust and ethics matter.
Timeline & Effort
▪ This is an ongoing stage that effectively starts early (even as soon as pilots are
conceived) and continues through deployment and beyond. However, a focused
change program for a specific AI rollout can run 4–8 weeks around the go-live
– with communications ramping up a few weeks before deployment and follow-
up after. Up-skilling initiatives can be phased over months or even years
(depending on the depth of skill gaps).
▪ The effort is significant in terms of stakeholder time – leadership must dedicate
time to communications, managers to training their teams, etc. It’s often useful to
align major AI adoption efforts with HR’s cycle (for instance, include AI readiness
goals in performance plans or tie into annual training calendars).

Step 8: AI Continuous Innovation

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: AI Continuous Innovation
85
Deliverables
▪ AI CoE Charter & Operating Model: Defines the mission, scope, &
structure of the AI CoE. Outlines responsibilities (governance, enablement,
project support), roles (e.g. data scientists, ML engineers), funding model
(central vs. charge-back), and engagement with business units.
▪ Continuous Improvement Plan: Schedule for monitoring and updating
production models. Includes an MLOps dashboard and periodic reports
(e.g. monthly) on performance, retraining, and key KPIs (accuracy, drift,
uptime, usage).
▪ Innovation Pipeline Backlog: A dynamic list of AI project ideas with
estimated value & status (e.g. idea, pilot). Reviewed regularly to guide
prioritization and planning.
▪ R&D Experiment Reports: Brief summaries of tests or research to
capture lessons learnt and to keep leadership informed.
▪ Standardized Tools & Knowledge Base: Shared code libraries, APIs,
datasets, & an internal wiki of reusable components, model metadata, and
best practices to streamline development & avoid duplication.
▪ Talent Development Programs: Initiatives like AI academy, hackathons,
or education partnerships. Deliverables include training outlines, MOUs, or
prototypes from internal events—supporting skill growth and innovation.
Key Activities & Tasks
▪ Establish AI Centre of Excellence (CoE): Create a central team to drive AI adoption,
maintain standards, evaluate new ideas, & ensure alignment with strategic and ethical
goals. The CoE coordinates cross-functional efforts, prevents duplication, & acts as internal
AI consultants.
▪ MLOps & Lifecycle Management: Implement robust MLOps to monitor, retrain, & update
models continuously. Set schedules for model reviews, handle data drift, & retire outdated
models. Treat models as evolving assets that adapt to changing data & business needs.
▪ Pipeline of AI Opportunities: Maintain a steady flow of new AI use cases through ideation
sessions, hackathons, & discovery workshops. Keep a prioritized backlog of ideas, with
some advancing to prototypes or pilots, enabling AI to expand gradually across functions.
▪ Tech Watch & R&D: Stay ahead by tracking emerging AI tools and methods (e.g. AutoML,
XAI, federated learning). Assign a team to test innovations & partner with universities,
startups, or consortia. This ensures the organization leverages cutting-edge advancements.
▪ Scale & Replicate Success: Codify proven AI solutions for reuse across business units.
Develop templates, playbooks, & reference architectures. Promote internal knowledge
sharing via roadshows or team rotations to accelerate broader adoption.
▪ Governance Evolution: Refine AI governance regularly based on experience and
regulatory changes. The CoE should update policies, monitor adherence, and ensure
ongoing compliance and ethical use across all AI initiatives.
▪ Talent & Culture: Build AI capability through defined career paths, training programs, and
mentoring. Foster a culture of experimentation and sharing. Use recognition programs to
reward innovation and encourage grassroots AI ideas across all levels.

Confidential | www.tha kra lone.co m
Enterprise AI Adoption: AI Continuous Innovation
86
Resources Involved (Vendor & Client)
▪ Vendor: In continuous mode, vendors play a smaller role but may still contribute
via periodic advisory (e.g. quarterly check-ins, sharing external benchmarks).
Initially managed services or short-term experts to assess complex technologies
is advised. If internal capacity is lacking, some companies temporarily outsource
parts of the CoE to consulting partners while building in-house capabilities.
▪ Client: Continuous innovation is largely client-led, embedded into daily
operations. Core contributors include AI CoE team members—data scientists,
ML/data engineers, product managers, and business translators from across
units. Senior executive sponsorship ensures alignment with business strategy.
Ongoing IT and risk team involvement maintains architecture and ethical
standards. Business unit liaisons (e.g. Stage 7 AI champions) regularly engage
the CoE to propose and drive new projects.
Key Outcomes
▪ Sustainable AI capability: The enterprise shifts from isolated wins to a repeatable,
sustainable AI engine. A well-established AI CoE ensures expertise is retained,
developed, & scaled. The company consistently delivers new AI solutions while
maintaining existing ones, a sign of mature AI adoption.
▪ Ongoing business value: With a live innovation pipeline, AI continues to drive
business impact. Rather than stalling post-pilots, new use cases emerge, boosting
efficiency, customer experience, and even revenue. AI remains a long-term
competitive differentiator, not a one-time project.
▪ Agility and innovation culture: A strong CoE and continuous learning structures
enhance adaptability. The enterprise can quickly test and adopt new AI tech,
staying ahead of industry shifts. Employees recognize the company’s commitment
to innovation, which boosts engagement and helps attract top talent.
▪ Proactive risk and compliance: Continuous governance prevents drift and
manages risks as AI scales. Ongoing monitoring and ethical reviews provide
assurance, giving the company confidence to explore ambitious AI initiatives,
knowing oversight is in place.
▪ Step 8 embeds AI into the enterprise DNA, moving from project to capability. The
organization becomes AI-driven, gaining compounding returns from optimized
solutions & rapid innovation. It’s positioned to evolve with AI’s future, adapting to
both incremental & disruptive changes.
Timeline & Effort
▪ This step is ongoing by nature. However, initial setup of structures like the AI
CoE can be done in 4–6 weeks for planning and approval, followed by
hiring/assigning staff over a few months. After that, continuous improvement
cycles might operate in quarterly sprints (e.g. quarterly pipeline review, model
audit every quarter, etc.) and annual strategy refreshes.
▪ The effort is about integrating AI into the regular management cadence of the
organization. It should eventually be seen less as a separate “stage” and more as
part of business-as-usual in the tech and innovation portfolio. However, dedicated
resources and time must be allocated; continuous innovation in AI is an
investment (like R&D) that pays off in staying competitive.

Appendix 2: Case Studies

Leading Utility Company in
Southeast Asia
Analytics
Product : Enterprise Lakehouse
Platform Implementation
Business Line : IT
Confidential | www.tha kra lone.co m
Enterprise Lakehouse Platform Implementation
88
Output
The engagement culminates in a comprehensive and actionable blueprint for delivering a unified, cloud-native
analytics platform that not only addresses current data fragmentation and integration challenges but also positions
the organization for future scalability, advanced analytics, and governance excellence. This outcome reflects a
balance between immediate technical implementation needs and long-term strategic objectives, ensuring that the
solution is robust, adaptable, and aligned with the client’s digital transformation goals.
Key deliverables include:
• SaaS-based Lakehouse for unified storage and analytics
• Real-time ERP integration using specialized extraction tools
• Enterprise data governance with cataloguing, lineage, and compliance controls
• Tiered data architecture for quality, consistency, and readiness
• Standardized pipelines for operational and external data sources
• Replatformed strategic reports using curated datasets
• Modernized ETL pipelines with automation, quality checks, and CI/CD
• Cutover validation between legacy and new ERP to reduce risks
• Governance framework aligning roles, resources, and communications
Overview
The client engaged Thakral One design and implement a modern cloud-based analytics platform that would unify
disparate data from core ERP and multiple operational systems. The solution adopts a three-tier architectural model
(Raw–Staged–Curated) to retain original datasets, transform and standardize them, and prepare optimized data
models for analytics and reporting. This initiative supports the client’s digital transformation by providing a single
source of truth and reducing integration complexity. Delivery is planned in three phases:
1. Foundation build with ERP and operational data integration into Raw/Staged layers,
2. Migration of management reporting and transformation of existing ETL logic into cloud-native pipelines,
3. Full integration with the next-generation ERP environment and structured post go-live validation.
4. An accelerated mobilization phase established governance, confirmed functional/technical requirements, and set
the execution roadmap.
Summary
Thakral One proposed and planned the
implementation of a cloud-based Enterprise
Lakehouse Platform for the client, aimed at
resolving data fragmentation, integration
complexity, and operational inefficiencies. The
solution will enable unified data access,
enhance decision-making capabilities, and
support the client’s digital transformation.

Real-Estate & Residential
Resort in Australia
Consulting
Product : Application Landscape and
Architecture Review
Business Line : Corporate
Timeline : 3.5 months
Team Size : 2
Confidential | www.tha kra lone.co m
Application Landscape and Architecture Review
89
Output
Thakral One created a Roadmap to help the company determine its next steps, including:
• Achieving organizational right-sizing and transfer full ownership of IT, setting up IT as a
centralized support hub with comprehensive strategies, policies, and processes.
• Implementing Business Continuity and Disaster Recovery plans, improve vendor management
standards, and enhance integration programs like SSO and MFA to boost efficiency and reduce
risks.
• Conducting thorough application reviews and evaluations to optimize critical software and
decommission outdated systems, ensuring best use of technology.
• Creating value by enhancing communication and organizational performance through training,
KPI initiatives, and the development of new initiatives such as a customer portal and a
centralized data repository with advanced business intelligence capabilities.
Overview
Through the assessment of the client’s Application Landscape and IT Architecture, the following
challenges were identified:
o Limited IT control over assets
o Insufficient documentation
o Understaffed, under-skilled IT team
o Limited value beyond data reporting
As such, Thakral One's Roadmap aims to address these issues and recommend improvements
to build the Company's IT infrastructure, streamline processes, enhance integration, and elevate
data management to improve overall business value.
Summary
Thakral One reviewed the client's
Application Landscape and Architecture
and reviewed its existing IT processes to
align with industry best practices.

Leading Credit Reporting
Agency in Southeast Asia
Consulting
Product : Architecture Review &
Cloud Readiness
Business Line : Credit Reporting
Timeline : 2.5 Months
Team Size : 3
Confidential | www.tha kra lone.co m
Optimizing Architecture for Cloud Readiness & Performance
90
Output
The assessment conducted by Thakral One identified key areas for improvement and provided actionable
recommendations to optimize the system for performance and cloud readiness.
• Authentication: Recommendations were made to streamline authentication and improve login speed.
• Code Reusability: Identified areas for improvement, such as enhancing logging practices and
separating tightly coupled services. With these, many services could be adapted for future use.
• Performance: Recommended further performance testing with multiple users and optimising data
handling.
• APM Tool Insights: An Application Performance Management tool was installed to provide detailed
performance insights.
• Automated Testing: Thakral One utilized jMeter for load testing, demonstrating its effectiveness for
future performance evaluations. This provided a framework for automated testing and benchmarking.
• Logging and Audit Trails: Inconsistencies in logging practices were noted, and standardization was
recommended. Additionally, moving audit logging to backend services was suggested to enhance
accuracy and security.
• Cloud Readiness: Thakral One highlighted the need for simplifying the architecture to better align with
cloud integration. Recommendations included testing service scalability, adjusting VM resources, and
preparing for containerization to ensure a smooth transition to the cloud and optimal performance.
Overview
The Thakral One team was engaged to review and enhance the architecture of the Credit
Management Revamp project's Microservices. The primary objective was to ensure that the
system adhered to best practices and to identify any performance issues. Thakral One
conducted a thorough analysis of the provided documentation, participated in Q&A sessions,
and performed a comprehensive code review. A key focus was evaluating how effectively the
system could be adapted for a cloud environment and addressing any performance and
usability concerns.
Summary
Thakral One reviewed and enhanced the
Credit Management Revamp project’s
microservices architecture. The team
focused on performance, cloud readiness,
and best practices. Its assessment
identified key improvements, provided
actionable recommendations, and
emphasized on optimizations.

Top Bank in the Philippines
Consulting
Product : Campaign Management
Platform Implementation
Governance
Business Line : Banking
Timeline : 4 Months
Team Size : 15
Confidential | www.tha kra lone.co m
Campaign Management Platform Implementation Governance
91
Output
Thakral One's role in this project is focused on providing governance and project management support to
ensure successful implementation and integration of the Campaign Management Platform.
Key responsibilities include:
• Governance: Establishing and maintaining project governance frameworks to ensure adherence to
timelines, quality standards, and regulatory requirements.
• Project Management: Overseeing all project activities, coordinating between the client and
implementation teams, and managing project schedules, resources, and risks.
• Quality Assurance Oversee: Ensuring that all deliverables, including the installation and configuration of
Pega CDH, data integration, and system testing, meet the client’s requirements and industry best
practices.
• Stakeholder Management: Facilitating communication and collaboration among stakeholders, providing
regular updates, and ensuring that project objectives align with the client’s business goals.
• Support and Transition: Guiding the back through the user acceptance testing (UAT) process,
supporting live deployment to production, and managing the warranty period post-Go-Live to address
any issues that may arise.
Through these governance and project management efforts, Thakral One ensures the project's successful
execution and alignment with the company’s strategic objectives.
Overview
The Pega Campaign Management Platform project aims to empower the bank’s consumer
business sector to effectively engage and interact with customers by launching automated,
timely, and relevant marketing campaigns. The platform will leverage spend triggers and
account behaviors to automate campaign triggers and marketing offers across various
channels, including SMS, email, and mobile apps. This initiative seeks to enhance customer
experiences and improve conversion rates by providing the bank with the tools to develop real-
time, personalized marketing offers based on specific spend, payment, or profile-based triggers.
Summary
Thakral One provided governance and
project management support for the
implementation of the Pega Campaign
Management Platform, ensuring adherence
to timelines, quality standards, and
regulatory requirements. Its responsibilities
include project oversight, quality assurance,
stakeholder management, and support.

Thrift Bank in the Philippines
Consulting
Product : Solution Development &
Vendor Selection
Business Line : Banking
Timeline : 6 Months
Team Size : 7
Confidential | www.tha kra lone.co m
Core Technology Solution Development & Vendor Selection
92
Output
Thakral One provided the following deliverables to the Bank’s Senior Management:
• Core Banking System Evaluation: Thakral One assessed the existing system, aligning it
with business needs and defining architectural and integration requirements.
• Business Requirements Definition: They facilitated sessions to identify and prioritize
business requirements for the new system.
• Architecture and Integration Analysis: Collaborative design sessions were held to outline
the new system’s architecture and address integration challenges.
• Implementation and Deployment Planning: A preliminary implementation plan was
developed, detailing methodologies, milestones, and risk assessments.
• Vendor Evaluation and Selection: Thakral One defined vendor selection criteria and
created an evaluation matrix, including drafting and managing the RFP process.
• Cloud Readiness and Scalability Assessment: Evaluated vendor solutions for SaaS
performance and cloud scalability to ensure the product could meet the Bank’s growth
needs. Assessed the scalability and flexibility of solution features. Reviewed the cloud
infrastructure’s capacity to handle increased workloads and maintain high performance,
ensuring the new system could scale efficiently in response to demand.
Overview
The bank engaged Thakral One to provide Vendor Sourcing and Evaluation Consulting to
facilitate the replacement of its core banking system. The goal was to ensure the new system
aligned with business and technical requirements, integrated seamlessly with existing systems,
and supported future scalability. Thakral One managed the end-to-end process, including writing
the RFP, evaluating vendors, and defining selection criteria.
Summary
Thakral One supported the bank in
replacing its core banking system by
managing vendor sourcing and evaluation.
It ensured the new system met business
needs, integrated with existing systems,
and supported future scalability.

Commercial Bank in the
Philippines
Analytics
Product : Microsoft Azure & Power BI
Business Line : Banking
Confidential | www.tha kra lone.co m
Analytics Data Management & Visualization
93
Output
The result was the operationalization of a robust data management solution that's easy to use
that can readily provide data. Foundation was also built for a data repository for advanced
analytics. This was possible through the following solutions:
• Design of the over-all data architecture required to realize the Customer 360 view including
the subject areas of Deposits, Credit Cards, Auto Loans and Mortgage Loans
• Analysis, Design and Implementation of the Data Management Solution
• Implementation of a full cloud solution using Microsoft Azure and Power BI
• Training Services on data management and visualization
Overview
Thakral One reduced manual processes in summarizing and visualizing data, and mitigated
latency issues because of data gathering and reporting. It also provided executives with the
data necessary for informed decision-making. Thakral One's implementation of a Customer 360
view, encompassing critical subject areas such as Deposits, Credit Cards, Auto Loans, and
Mortgage Loans, laid the foundation for advanced analytics, including customer segmentation,
cross-sell/up-sell modeling, churn prediction, and campaign analytics. The following problem
was uncovered:
• Lack of a centralized and streamlined solution in the preparation and dissemination of data
used for reporting and analysis
Summary
Thakral One addressed the customer's
issue of manual data processing and
reporting latency by providing a centralized
and streamlined data management solution
using Microsoft Azure and Power BI,
enabling the client to maximize data assets
and lay the foundation for advanced
analytics and customer insights.

PH Government Agency
Analytics
Product : Supporting the Implementation
of an Asset Information
Management System
Business Line : Government
Confidential | www.tha kra lone.co m
Supporting the Implementation of an Information System
94
Output
Thakral One was able to complete data migration into the central processing storage platform,
and consolidated taxpayer information by developing business rules and reporting templates.
The development of dashboards was possible through improved insighting. This was achieved
through the following solutions:
• Creation of business rules
• Transaction corrections, adjustments, and transformation
• Configuration of data availability and retention policies
• User access and rights configuration
• Frequency design of data mart refresh
• Facilitation of an end-user training course
Overview
Thakral One was engaged by a government agency to enhance data sharing between various
government agencies to establish a comprehensive taxpayer asset database. The goal is to
streamline tax administration processes and facilitate more informed tax policy analysis. The
following gaps were uncovered:
• Each government agency has its own data formats, systems, and levels of data quality
• Data from different sources requires complex standardization efforts
• Differing priorities, protocols, and information management systems across the various
agencies.
Summary
Thakral One supported the implementation
of an integrated data management solution
in the areas of data migration, reporting,
and visualization.

Large MFI in Cambodia
Analytics
Product : Analytics Roadmap Design
& Implementation
Business Line : Banking
Confidential | www.tha kra lone.co m
Analytics Roadmap Design & Implementation
95
Output
Thakral One was able to optimize data warehousing and data management processes for
enhanced reporting and analytics. It was also able to define sales and marketing strategies that
could address growth opportunities using the 360 data model and customer segmentation as
the pilot use cases. This was achieved through the following solutions:
• Business analytics advisory services
• Analytics roadmap creation
• Customer 360 data model and segmentation analysis application
Overview
Thakral One was engaged by a large MFI in Cambodia to establish a deeper understanding of
analytics and its potential benefits. It also assessed the client's current landscape and
recommended an analytics roadmap to support its business goals, as well as defined pilot use
cases. During the engagement, the following gaps were uncovered:
• Lack of structured and optimized data warehouse and data management processes for
reporting and analytics activities.
• No existing model to understand customer activity and validate business assumptions that
would help in business strategy formulation
• Insufficient appreciation of the benefits of analytics which are essential to support the
initiatives of the client’s data scientists
Summary
Thakral One provided advisory services to
help an MFI appreciate the value of
analytics and apply it in strategy formulation
to address business goals and growth
objectives.

Top Local Credit Card Provider
in the PH
Analytics
Product : Customized Marketing &
Customer Management
Business Line : Banking
Confidential | www.tha kra lone.co m
Customized Marketing & Customer Management
96
Output
Thakral One was able to enable the client to deeply understand their customers and launch
customized campaigns to their segments, increasing loyalty and profitability. The segmentation
model was used for years, supporting more personalized marketing strategies and preparing
them to adopt the next step of implementing a Campaign Management platform. This was
achieved through the following solutions:
• Provided the client with consulting and model development services in creating a behavioral
customer segmentation model leveraging on its existing SAS Analytics Licenses
Overview
Thakral One was engaged by a top local credit card provider in the PH to generate more
innovative marketing and customer management strategies to maintain its competitive market
position and further improve on its offers and promotions. During the engagement, the following
gaps were uncovered:
• A strong analytics team was present, but they were loaded with daily BAU tasks and required
fresh business perspectives to improve performance
Summary
Thakral One enabled a credit card provider
via consulting and model development to
better understand their customers, maintain
their market position, and enhance
profitability.

Leading Bank in Vietnam
Analytics
Product : Behavioral Segmentation
for a Bank’s Card Unit
Business Line : Banking
Confidential | www.tha kra lone.co m
Behavioral Segmentation for a Bank’s Card Unit
97
Output
Thakral One enabled the bank to create initiatives, programs, and new features for customer
engagements . Additionally, engagements have become focused and targeted with measurable
KPIs and success criteria . This was achieved through the following solution:
• Developed behavioral segmentation using Open-Source R. The results differentiated the
profile of active from inactive card users, improved the bank’s understanding of their
customers’ spending behavior between positive vs. negative, low vs. high tiers, and
transactors vs revolvers.
Overview
Thakral One was engaged by a leading bank in Vietnam to leverage on analytics to develop a
better understanding of customer behavior. It also helped develop programs to increase usage
and grow wallet share amongst cardholders, improve customer value proposition, and increase
uptake on campaign/product offerings . During the engagement, the following gaps were
uncovered:
• While the bank has a strong reporting / BI team, the business team managing the credit cards
requires complementary expertise in Analytics to be able to accomplish the needed customer
segmentation project.
Summary
Thakral One enabled a credit card provider
via consulting and model development to
better understand their customers, maintain
their market position, and enhance
profitability.

Major Hospital in Singapore
Analytics
Product : Developing an Interactive
COVID Monitoring
Dashboard
Business Line : Healthcare
Timeline : 2 Months
Team Size : 5
Confidential | www.tha kra lone.co m
Interactive COVID Monitoring Dashboard
98
Output
Thakral One was able to build the disease surveillance dashboard which enabled early
detection and intervention, reducing infections and fatalities by detecting COVID-19 hotspots
and facilitating quick interventions. It also improved contact tracing and enabled data-driven
decision-making, leading to efficient resource allocation and increased staff safety by providing
real-time information on infections. This was achieved through the following solutions:
• An Interactive Dashboard which allowed slicing and dicing the distribution of symptoms,
suspect cases, and confirmed cases across key areas within the facility and per
department.
• A Network Graph which showed the movement of infected staff across various locations
within the facility to detect any location-based relationships between cases and potential
exposures.
Overview
Thakral One was engaged by a major hospital in Singapore to track the incidence of staff -
related respiratory infections by building a disease dashboard. It was also tasked to identify, as
early as possible, potential COVID-19 hotspots in various facilities by tracking the movement of
medical staff. During the engagement, the following gaps were uncovered:
• High volume of data coming from a large number of medical staff present
• Medical staff work across various departments and wards within the healthcare facility,
making it challenging to track movement and potential exposure
Summary
A disease dashboard and surveillance tool
to enable the early identification of potential
COVID hotspots.

Top Telecommunications Comp
any in the PH
Analytics
Product : Forecasting Customer
Lifetime Value (CLV)
Business Line : Telecommunications
Confidential | www.tha kra lone.co m
Forecasting Customer Lifetime Value
99
Output
Thakral One was able to optimize data processing runtime of customers historical profit, fine-
tuned CLV forecasts over a 5-year period, compared baseline model and recalibrated model,
analyzed shifts in customer CLV bands as a result of the “pandemic period”, and increased
forecast accuracy. This was achieved through the following solutions:
• Delivered an end-to-end workflow that forecasts CLV of prepaid and postpaid subscribers
over a 5-year period.
• Provided a metric critical in the defining the telco’s customer segments based on profitability
and developing marketing strategies based on customer value.
• Improved data processing runtime and fine-tuned CLV estimates within the telco’s
acceptable forecast error rates.
Overview
Thakral One was engaged by a top telecommunications company in the Philippines to migrate
end-to-end calculation of CLV models of prepaid and postpaid subscribers from data
preparation to forecasting. It was also tasked to recalibrate CLV forecasting models with new
data. During the engagement, the following gaps were uncovered:
• Due to the growing size of the telco’s subscriber base, optimizations in data processing and
model fit was a major consideration for optimal runtime of scripts.
• Extreme and illogical forecasts needed to be treated to prevent over/underestimation of CLV.
Summary
Thakral One migrated CLV forecasting
models for prepaid and postpaid
subscribers of a telco in the Philippines from
SAS to open-source using Cloudera Data
Science Workbench.

PH Government Agency
Analytics
Product : Financial Crimes
Monitoring & Social
Network Analysis
Business Line : Government
Confidential | www.tha kra lone.co m
Financial Crimes Monitoring & Social Network Analysis
100
Output
Thakral One was able to improve identification and consolidation of customer entities from all
financial institutions. It also enabled the client to understand the administration, functionality,
and usage of the SAS FCM and SNA solutions, which were used to detect possible entities
engaging into fraudulent and laundering activities . This was achieved through the following
solutions:
• Developed a program that created a single entity view of customers across all financial
institutions in the country
• Implemented SAS Financial Crimes Monitor (the interface for the generation and
administration of fraud alerts) and SAS Social Network Analysis (the investigator interface for
viewing / managing alerts and customer network) solutions.
Overview
Thakral One was engaged by a government agency in the Philippines to create a single entity
view of all the financial customers of the country , and to uncover and detect hidden patterns of
fraud and laundering activities. During the engagement, the following gaps were uncovered:
• Client cannot depend on their manual investigation process to detect fraudsters and
launderers due to the growing volume of financial customers and their transactions data,
• Need for automatic detection of fraudulent transactions by going beyond transaction and
account views to analyze all related activities and relationships
Summary
Thakral One developed a program for a PH
financial regulator to have a single entity
view of financial customers and to improve
overall fraud monitoring performance.

Loan Approval for a Leading
Bank in Vietnam
Analytics
Product : Credit Decision Engine
Pilot Implementation
Business Line : Banking
Timeline : 7 Months
Team Size : 10
Confidential | www.tha kra lone.co m
Credit Decision Engine Pilot Implementation
101
Output
Thakral One was able to enable real time processing for a secured loan product , configured
the SAS solution to integrate w/ external and internal data, and implemented decision history
data capture for monitoring and analysis . Additionally, it trained bank staff to
maintain/maximize the system. This was achieved through the following solutions:
• Implementation of SAS Risk Modeling and Decisioning, starting with SAS Intelligent
Decisioning for automation and Real Time Processing, Model Manager for Credit Scoring
Model Ingestion and SAS Studio for Data Management
• Value-add solution ideation and implementation for testing data generation, transformation
and batch testing
• Development check-points and Knowledge Transfer
Overview
Thakral One was engaged by the bank to increase capacity for more loan applications , improve
operational efficiency for faster processing , and to establish credit scoring model and credit
decision strategy governance. During the engagement, the following gaps were uncovered:
• Collaboration points required between Thakral One as the main driver of implementing the
SAS component, and the owners of the Bank’s internal data sources and applications
• Integration Specification alignment between SAS and the Bank’s LOS
• Model Ingestion Approach for incorporating the Bank’s Credit Scoring Model
Summary
As part of the Bank’s endeavor to improve
operational efficiencies, scale up and match
the market’s increasing demand and better
manage risks, SAS Risk Modeling &
Decisioning became one of the core engines
providing empowerment for Decision Strategy
and Model Governance, and Automation.

Confidential | www.tha kra lone.co m
Multi-Year Data & Analytics Engagement
102
➢ Staff Augmentation for the development of 3 Campaign use cases
➢ ML/Advanced Analytics Model development
➢ Expansion of campaign development team
➢ Transition from Staff Augmentation to Managed Services
➢ Beginning of Campaign Operations Platform Support
YR 2
YR 3
YR 4
YR 5
YR 6
YR 7
➢ Further expansion and continued extension of Campaign Development, Analytics and Campaign
Operations
➢ Big Data POC
➢ Developer Automation for Campaigns as CI/CD Pilot
➢ Conduct of Customized Bootcamp
➢ Pivot to Open Source Platform
➢ Further Advanced Analytics and Data Insighting Support
➢ Continued Campaign Delivery Support
➢ Expanded Campaign Operations to full L1,L2,L3 Support Model
➢ Continued Advanced Analytics, Data Insighting & Campaign Delivery Support
➢ Cloud Migration
➢ Expanded Campaign Operations to full L1,L2,L3 Support Model
➢ Continued Advanced Analytics, Data Insighting & Campaign Delivery Support
➢ Further Cloud Adaption and Incorporation of new technologies
➢ ML Models Optimization
➢ Continuous Improvement and Automation Initiatives
➢ Cost Optimization Initiatives
➢ Campaign Execution Capacity Initiatives
➢ Focus on Strategic Value Generation Initiatives
YR 1
A Leading Telecommunications
in the Asia Pacific Region
Data & Analytics Delivery and
Campaign Operations Services
Product : Cloud & Open-Source
Business Line : Data Operations Division
Timeline : 6+ years
Summary
Thakral One has demonstrated its capability
to support customers in harnessing the
power of data, analytics, insighting,
technology and automation within the field
of Telecommunications and Customer
Experience

Telecommunications Company
in the Asia Pacific Region
Analytics
Product : Analytics Operationalization
and Implementation of Data
Driven Campaigns
Business Line : Telecommunications
Confidential | www.tha kra lone.co m
Analytics Operationalization and Data-Driven Campaigns
103
Output
Thakral One enabled analytics operationalization in its true essence, going beyond technical
model deployment, achieving commercial deployment of analytics-driven campaigns. It also
allowed for the implementation of a variety of event triggered and data driven campaigns.
Profiling of campaign fall-outs that were used as additional input in improving campaign
executions was also made possible through several campaign iterations. This was achieved
through the following solutions:
• Analytics model development and operationalization
• Campaign configuration and best practices
• Historical and event streaming data management
• Configuration of event-triggered real-time campaigns and customer service processes
• Campaign Reporting
• Environment stabilization and platform administration
• Application Support
Overview
Thakral One was engaged by the client to improve user experience , catch up with user demand
and provide more accessible channels , improve Customer Experience Management strategy ,
and maximize ROI to deliver better business impact. In the course of the engagement, the
following gaps were uncovered:
• Quickly expanding subscriber base
• Need for more complex but accessible user channels
• Need for an improved Customer Experience Management strategy
• Business solutions for real-time customer experience orchestration
Summary
Thakral One provided expertise in analytics
model development and operationalization,
implementation of data-driven campaigns,
and profiling of campaign fallouts.